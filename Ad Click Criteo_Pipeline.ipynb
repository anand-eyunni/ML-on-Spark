{"cells":[{"cell_type":"markdown","source":["# w261 - Project - Criteo Clickthrough\n#### Project Team:\n- Yekun Wang\n- Anand Eyunni\n- Dinesh Achuthan\n- Miroslava Walekova\n\n#### Contents\n 0. Notebook Set Up \n 1. Introduction\n 2. Exploratory Data Analysis\n 3. Preprocessing Numeric Variables\n 4. Algorithm Theory\n 5. Feature Engineering and Baseline Model\n 6. Model Training and Selection \n 7. Conclusions\n 8. Course Concepts\n \n#### Executive Summary\n\nCriteo is a marketing solution that enables online businesses to follow up visitors who have left a website without a transaction by using customized banners as ads. The solution operates on a pay per click/cost per click basis. Advertisers only pay for the customers that click on an ad banner. In this particular scenario, the problem we are trying to solve is to predict the likelihood that a user will click on a given ad on a page. The prediction is done based on features generated from traffic logs along with click labels.\n\nThe Criteo dataset has approximately more than 46 million records and contains 26 categorical variables and 13 numeric variables. We take the following steps to prepare our solution - data analysis, data preparation, feature engineering, model training and hyperparamter tuning. We perform exploratory data analysis on the given data and realize that there are missing, null, negative and hashed values. Without columnar description, it is hard to make meaningful assumptions about backfilling data. We perform downsampling and upsampling to maintain balance between the two outcomes. We also test for correlation between variables (and log transformed variables) and realized that multicolinearity was not an issue. We also perform imputation using column mean because it worked better than median imputation. \n\nWe then begin baseline modeling using logistic regression and computed the confusion matrix, ROC curve, and Precision vs Recall. We also explore normalization of numeric variables in the baseline model to search for improvement in the model. For categorical variables, we impute the missing values, and perform binning, hashing, one-hot-encoding and chi-squared feature selection. There is no significant impact on Precision, Recall, AUC or Logloss. Rebalancing the training dataset has an impact on all four metrics. The next model is a Random Forest with cross validation and evaluation of model performance. It performs less well than the logistic regression model. A third model in the form of Gradient Boosted Trees also performed less well than logistic regression despite hyper parameter tuning. Eventually, we settle on the logistic regression for our click predictions based on our evaluation metrics and kaggle leaderboard logloss metric."],"metadata":{}},{"cell_type":"markdown","source":["# 1. Notebook Set-Up"],"metadata":{}},{"cell_type":"code","source":["# IBM DagCrossValidator set up\n# %sh \n# git clone https://github.com/Walekova/PipelineTuning.git"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">fatal: destination path &#39;PipelineTuning&#39; already exists and is not an empty directory.\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["#%sh\n#ls -alh /databricks/driver/PipelineTuning/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">total 36K\ndrwxr-xr-x 4 root root 4.0K Apr 14 23:16 .\ndrwxr-xr-x 1 root root 4.0K Apr 15 02:06 ..\ndrwxr-xr-x 8 root root 4.0K Apr 14 23:16 .git\n-rw-r--r-- 1 root root  18K Apr 14 23:16 pipeline_tuning.py\ndrwxr-xr-x 2 root root 4.0K Apr 14 23:16 __pycache__\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["import sys\n\n# Add the path to system\n#sys.path.append('/databricks/driver/')\n#sys.path.append('/databricks/driver/PipelineTuning/')\n#sys.path.append('/databricks/driver/PipelineTuning/pipeline_tuning.py')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Imports\n\n# Utilities\nimport re\nimport time\nimport ast\nimport os\nimport tarfile\nimport itertools\n\n# Transformation\nimport numpy as np\nimport pandas as pd\nfrom functools import reduce\n\n# Plotting\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\nimport seaborn as sns\n%matplotlib inline\n\n# Spark SQL Imports\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, count as sparkcount, when, lit\nfrom pyspark.sql import DataFrame\n\n# Spark ML imports - pipeline components\nfrom pyspark.ml.feature import Imputer, StringIndexer, VectorAssembler, StandardScaler\nfrom pyspark.ml.feature import FeatureHasher, OneHotEncoderEstimator, ChiSqSelector\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Spark ML imports - modelling metrics\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.classification import GBTClassifier\n# from pyspark.mllib.evaluation import MulticlassMetrics ### reguired for F1 score calculation\n\n# Spark ML imports - models\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# Custom IBM Codait pipeline tuning library\n# from pipeline_tuning import DagCrossValidator ### -> build for python 2.7"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Get user home directory\nusername = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\nuserHome = 'dbfs:/user/' + username\nprojectPath = userHome + \"/project/\" \n\n# Extract source data from tar file\nprojectPathOpen = '/dbfs' + projectPath.split(':')[-1] \ndbutils.fs.mkdirs(projectPath)\nsource_data = '/dbfs/mnt/mids-w261/data/datasets_final_project/dac.tar.gz'\ntar = tarfile.open(source_data, \"r:gz\")\ntar.extractall(projectPathOpen)\ntar.close()\n\n# Read and convert Train Data into parquet format\ndbutils.fs.rm(projectPath+\"/dataTrain.parquet\", recurse=True)\n\nschema = StructType([\\\n      StructField(\"label\", FloatType(), True),\\\n      StructField(\"I1\", FloatType(), True),\\\n      StructField(\"I2\", FloatType(), True),\\\n      StructField(\"I3\", FloatType(), True),\\\n      StructField(\"I4\", FloatType(), True),\\\n      StructField(\"I5\", FloatType(), True),\\\n      StructField(\"I6\", FloatType(), True),\\\n      StructField(\"I7\", FloatType(), True),\\\n      StructField(\"I8\", FloatType(), True),\\\n      StructField(\"I9\", FloatType(), True),\\\n      StructField(\"I10\", FloatType(), True),\\\n      StructField(\"I11\", FloatType(), True),\\\n      StructField(\"I12\", FloatType(), True),\\\n      StructField(\"I13\", FloatType(), True),\\\n      StructField(\"C1\", StringType(), True),\\\n      StructField(\"C2\", StringType(), True),\\\n      StructField(\"C3\", StringType(), True),\\\n      StructField(\"C4\", StringType(), True),\\\n      StructField(\"C5\", StringType(), True),\\\n      StructField(\"C6\", StringType(), True),\\\n      StructField(\"C7\", StringType(), True),\\\n      StructField(\"C8\", StringType(), True),\\\n      StructField(\"C9\", StringType(), True),\\\n      StructField(\"C10\", StringType(), True),\\\n      StructField(\"C11\", StringType(), True),\\\n      StructField(\"C12\", StringType(), True),\\\n      StructField(\"C13\", StringType(), True),\\\n      StructField(\"C14\", StringType(), True),\\\n      StructField(\"C15\", StringType(), True),\\\n      StructField(\"C16\", StringType(), True),\\\n      StructField(\"C17\", StringType(), True),\\\n      StructField(\"C18\", StringType(), True),\\\n      StructField(\"C19\", StringType(), True),\\\n      StructField(\"C20\", StringType(), True),\\\n      StructField(\"C21\", StringType(), True),\\\n      StructField(\"C22\", StringType(), True),\\\n      StructField(\"C23\", StringType(), True),\\\n      StructField(\"C24\", StringType(), True),\\\n      StructField(\"C25\", StringType(), True),\\\n      StructField(\"C26\", StringType(), True)])      \n\ntxtTrain = spark.read.format(\"csv\")\\\n.option(\"header\", \"false\")\\\n.option(\"delimiter\", \"\\t\")\\\n.schema(schema)\\\n.load(projectPath+\"train.txt\")\n\ntxtTrain.write.format(\"parquet\").save(projectPath+\"/dataTrain.parquet\")\n\n# Read and convert Test Data into parquet format\n\ndbutils.fs.rm(projectPath+\"dataTest.parquet\", recurse=True)\n\nschema = StructType([\\\n      StructField(\"I1\", FloatType(), True),\\\n      StructField(\"I2\", FloatType(), True),\\\n      StructField(\"I3\", FloatType(), True),\\\n      StructField(\"I4\", FloatType(), True),\\\n      StructField(\"I5\", FloatType(), True),\\\n      StructField(\"I6\", FloatType(), True),\\\n      StructField(\"I7\", FloatType(), True),\\\n      StructField(\"I8\", FloatType(), True),\\\n      StructField(\"I9\", FloatType(), True),\\\n      StructField(\"I10\", FloatType(), True),\\\n      StructField(\"I11\", FloatType(), True),\\\n      StructField(\"I12\", FloatType(), True),\\\n      StructField(\"I13\", FloatType(), True),\\\n      StructField(\"C1\", StringType(), True),\\\n      StructField(\"C2\", StringType(), True),\\\n      StructField(\"C3\", StringType(), True),\\\n      StructField(\"C4\", StringType(), True),\\\n      StructField(\"C5\", StringType(), True),\\\n      StructField(\"C6\", StringType(), True),\\\n      StructField(\"C7\", StringType(), True),\\\n      StructField(\"C8\", StringType(), True),\\\n      StructField(\"C9\", StringType(), True),\\\n      StructField(\"C10\", StringType(), True),\\\n      StructField(\"C11\", StringType(), True),\\\n      StructField(\"C12\", StringType(), True),\\\n      StructField(\"C13\", StringType(), True),\\\n      StructField(\"C14\", StringType(), True),\\\n      StructField(\"C15\", StringType(), True),\\\n      StructField(\"C16\", StringType(), True),\\\n      StructField(\"C17\", StringType(), True),\\\n      StructField(\"C18\", StringType(), True),\\\n      StructField(\"C19\", StringType(), True),\\\n      StructField(\"C20\", StringType(), True),\\\n      StructField(\"C21\", StringType(), True),\\\n      StructField(\"C22\", StringType(), True),\\\n      StructField(\"C23\", StringType(), True),\\\n      StructField(\"C24\", StringType(), True),\\\n      StructField(\"C25\", StringType(), True),\\\n      StructField(\"C26\", StringType(), True)])   \n\ntxtTest = spark.read.format(\"csv\")\\\n.option(\"header\", \"false\")\\\n.option(\"delimiter\", \"\\t\")\\\n.schema(schema)\\\n.load(projectPath+\"test.txt\")\n\ntxtTest.write.format(\"parquet\").save(projectPath+\"/dataTest.parquet\")\n\n# secondary dataframes\ntrain_parquet = spark.read.parquet(projectPath+\"dataTrain.parquet\")\ntest_parquet = spark.read.parquet(projectPath+\"dataTest.parquet\")\n\n# check directory\ndisplay(dbutils.fs.ls(projectPath))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Raw data files were provided to us as a .tar.gz package. We first extracted all files from the .tar.gz package and converted data files in to parquet files. We chose parquet as the data format because its columnar fomat results in high performance and efficiency in both storage and processing. It is especially useful for later EDA stages when we have to do a column lookup to examine each feature individually."],"metadata":{}},{"cell_type":"code","source":["# Initialise Spark\nsc = spark.sparkContext\nsqlContext = SQLContext(sc)\n\n# Clear cache - helps when rerunning the workbook \nsqlContext.clearCache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["# 2. Introduction\n\n### 2.1 Document Purpose\n\nClick through rate (CTR) is a metric that measures the number of clicks that a publisher receives on the ads per number of impressions. Machine learning plays a central role in computing the expected CTR of ad impressions and computing advertising campaign ROI. \n\nThe following paper:\n- outlines the methodology applied to feature pre-processing and model evaluation techniques\n- defines a basic machine learning pipeline to enable execution and evaluation of individual solution experiments\n- introduces several machine learning modelling options and evaluates them using pre defined metrics\n\n### 2.2 Dataset Contents\n\nThe train and test data combined consists of more than 46 million rows. Each row corresponds to a display ad served by Criteo. Criteo is a company focused on bringing machine-learning technology, data and performance at scale together to drive measurable ROI.\n\nPositive (clicked) and negatives (non-clicked) examples have both been subsampled at different rates in order to reduce the dataset size. The examples are chronologically ordered. The training data consists of a portion of Criteo's traffic over a period of 7 days and the test set is computed in the same way as the training set but for events on the day following the training period.\n\nData Fields:\n- Label - Target variable that indicates if an ad was clicked (1) or not (0).\n- I1-I13 - A total of 13 columns of integer features.\n- C1-C26 - A total of 26 columns of categorical features hashed onto 32 bits.\n\n### 2.3 Problem Statement\n\n- Given the available information for a single ad impression, what is the likelihood of it results in a positive (clicked) event? \n- Which features are useful in predicting a click on a display ad?\n- Which machine learning model is model suitable to predict clickthrough on an display ad for the given data scale? \n\n### 2.4 Evaluation of Metrics\n\nWe have decided to use a collection of metrics to evaluate our solution to the Criteo problem. This is because each of these metrics provides us with a different insight into the observed model output.\n\n#### LogLoss\n \nOur initial intention was to use logloss to calculate the binary cross entropy. Log Loss takes into account the uncertainty of the prediction based on how much it varies from the actual label - entropy. However it is sensitive to imbalanced datasets. \n\n$$ LogLoss = - \\frac{1}{n} \\sum_{i=1}^{n}[y_i \\times log(\\hat{p_i}) + (1 - y_i) \\times log(1 - \\hat{p_i})]  $$ <br>\n\n#### Area Under the Curve (AUC)\n\nAUC maximizes the model's ability to discriminate between classes whilst the logloss penalizes the divergency between actual and estimated probabilities. While we cannot say that a model maximizing AUC means minimized log loss. Whether a model minimizing log loss corresponds to maximized AUC will rely heavily on the context; class separability, model bias.\n\nAUC is scale-invariant. It measures how well predictions are ranked, this makes it suitable for click advertising campaign. AUC is a good metric to use since the predictions ranked by probability is the order in which a list of adverts can be displayed.\n\nAs model maximising AUC may not lead to a model minimising Logloss we have decided to use AUC as our model evaluation metric, however also calculate Logloss as a control metric to evaluate our progress in building our model.\n\n#### Confusion Matrix, Precision & Recall\n\nIn addition to AUC and Logloss we have decided to capture the confusion matrix, precision and recall because of their interpretability characteristics. \n\n#### Other Metrics Considered\n\nOther metrics that could be used to evaluate results for a CTR modelling problem are F Score and Payoff matrix.\n\n__Adjusted F Score__\n\nRegular F1 score gives equal weight to to precision and recall. However in the case of click through rate this woud not be appropriate. We would need to include domain knowledge in our evaluation, to decide the appropriate weighting for recall or precision.\n\nWeighted F1 metric, where beta manages the tradeoff between precision and recall:\n\n$$ F_{beta} = (1 + \\beta^2) \\times \\frac{precision \\times recall}{\\beta^2 * precision + recall} $$\n\n__Payoff__\n\nConfusion matrix can be used to calculate payoffs.\n\n$$ Payoff = a \\times TP + b \\times FP + c \\times FN + d \\times TN  $$\na - Revenue from clicks <br>\nb - Cost of clicks that do not result in revenue <br>\nc - Opportunity cost <br>\nd - No cost or revenue <br>\n\nWe do not however have sufficient domain knowledge and  information about the source data industry hence we are unable to make any assumptions in relation to calculation of the payoff matrix.\nThis is because in advertising, majority of the charging models are set at cost per impression rather than cost per click."],"metadata":{}},{"cell_type":"markdown","source":["# 3. Exploratory Data Analysis"],"metadata":{}},{"cell_type":"markdown","source":["The following two SQL statements provide a glimpse into the raw data sets. Notice that There are many NULL values across both numeric and categorical variables."],"metadata":{}},{"cell_type":"code","source":["# Load source data into SQL table for queries\ntrainPath = projectPath+'dataTrain.parquet/'\ntestPath = projectPath+'dataTest.parquet/'\nspark.sql(\"DROP TABLE IF EXISTS trainTable\")\nspark.sql(\"DROP TABLE IF EXISTS testTable\")\nspark.sql(\"CREATE TEMPORARY TABLE trainTable USING parquet OPTIONS (path '{}')\".format(trainPath))\nspark.sql(\"CREATE TEMPORARY TABLE testTable USING parquet OPTIONS (path '{}')\".format(testPath))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[250]: DataFrame[]</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["Through EDA we intend to analyze these features for:\n1.\tMissing values\n2.\tErroneous and or outliers\n3.\tUnderstand the unit representation of the data to enable the decision on normalization or standardization in the feature engineering phase\n4.\tConstant features ( zero variance in a feature across the data set)"],"metadata":{}},{"cell_type":"code","source":["# View raw training data\n%sql select * from trainTable limit 10;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>label</th><th>I1</th><th>I2</th><th>I3</th><th>I4</th><th>I5</th><th>I6</th><th>I7</th><th>I8</th><th>I9</th><th>I10</th><th>I11</th><th>I12</th><th>I13</th><th>C1</th><th>C2</th><th>C3</th><th>C4</th><th>C5</th><th>C6</th><th>C7</th><th>C8</th><th>C9</th><th>C10</th><th>C11</th><th>C12</th><th>C13</th><th>C14</th><th>C15</th><th>C16</th><th>C17</th><th>C18</th><th>C19</th><th>C20</th><th>C21</th><th>C22</th><th>C23</th><th>C24</th><th>C25</th><th>C26</th></tr></thead><tbody><tr><td>0.0</td><td>28.0</td><td>3.0</td><td>12.0</td><td>44.0</td><td>15.0</td><td>43.0</td><td>28.0</td><td>43.0</td><td>44.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>32.0</td><td>05db9164</td><td>2f659110</td><td>e5cdf789</td><td>c2fcecf6</td><td>4cf72387</td><td>null</td><td>a4b6564e</td><td>1f89b562</td><td>a73ee510</td><td>3b08e48b</td><td>a04db730</td><td>72ac7802</td><td>c66b30f8</td><td>b28479f6</td><td>daf40beb</td><td>f1a11ae6</td><td>e5ba7672</td><td>c84cfab2</td><td>21ddcdc9</td><td>5840adea</td><td>f8709c5b</td><td>null</td><td>32c7478e</td><td>38be899f</td><td>f55c04b6</td><td>56009c93</td></tr><tr><td>0.0</td><td>null</td><td>-1.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>4.0</td><td>4.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>be589b51</td><td>8084ee93</td><td>02cf9876</td><td>c18be181</td><td>25c83c98</td><td>7e0ccccf</td><td>08383675</td><td>5b392875</td><td>7cc72ec2</td><td>3b08e48b</td><td>727af3e2</td><td>8fe001f4</td><td>49fe3d4e</td><td>07d13a8f</td><td>422c8577</td><td>36103458</td><td>2005abd1</td><td>52e44668</td><td>null</td><td>null</td><td>e587c466</td><td>null</td><td>be7c41b4</td><td>3b183c5c</td><td>null</td><td>null</td></tr><tr><td>0.0</td><td>1.0</td><td>260.0</td><td>128.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>8.0</td><td>11.0</td><td>11.0</td><td>1.0</td><td>6.0</td><td>null</td><td>0.0</td><td>68fd1e64</td><td>38a947a1</td><td>8512d54d</td><td>7031bb66</td><td>25c83c98</td><td>7e0ccccf</td><td>c9f171f9</td><td>5b392875</td><td>a73ee510</td><td>e4706565</td><td>755e4a50</td><td>e2163351</td><td>5978055e</td><td>b28479f6</td><td>46ed0b3c</td><td>beef16d6</td><td>d4bb7bd8</td><td>2c6cb693</td><td>null</td><td>null</td><td>4645d72c</td><td>null</td><td>bcdee96c</td><td>b258af68</td><td>null</td><td>null</td></tr><tr><td>1.0</td><td>8.0</td><td>0.0</td><td>7.0</td><td>12.0</td><td>2.0</td><td>7.0</td><td>9.0</td><td>3.0</td><td>29.0</td><td>1.0</td><td>2.0</td><td>null</td><td>6.0</td><td>5bfa8ab5</td><td>287130e0</td><td>95543f64</td><td>e326e2dc</td><td>43b19349</td><td>13718bbd</td><td>6284da2d</td><td>0b153874</td><td>a73ee510</td><td>0a263d38</td><td>5874c9c9</td><td>67f9c091</td><td>740c210d</td><td>1adce6ef</td><td>310d155b</td><td>a31938ae</td><td>e5ba7672</td><td>891589e7</td><td>4a237258</td><td>b1252a9d</td><td>fc87b483</td><td>null</td><td>bcdee96c</td><td>3fdb382b</td><td>ea9a246c</td><td>49d68486</td></tr><tr><td>0.0</td><td>0.0</td><td>44.0</td><td>56.0</td><td>1.0</td><td>1751.0</td><td>5.0</td><td>17.0</td><td>3.0</td><td>57.0</td><td>0.0</td><td>7.0</td><td>0.0</td><td>1.0</td><td>68fd1e64</td><td>287130e0</td><td>35e8c904</td><td>38c6a2ff</td><td>25c83c98</td><td>7e0ccccf</td><td>fed3cb1d</td><td>0b153874</td><td>a73ee510</td><td>175d6c71</td><td>b7094596</td><td>ca9f3db8</td><td>1f9d2c38</td><td>07d13a8f</td><td>10040656</td><td>d8daf836</td><td>e5ba7672</td><td>891589e7</td><td>21ddcdc9</td><td>5840adea</td><td>0bf4a9b7</td><td>ad3062eb</td><td>32c7478e</td><td>3fdb382b</td><td>ea9a246c</td><td>49d68486</td></tr><tr><td>0.0</td><td>2.0</td><td>7.0</td><td>35.0</td><td>10.0</td><td>82.0</td><td>31.0</td><td>41.0</td><td>20.0</td><td>289.0</td><td>1.0</td><td>13.0</td><td>0.0</td><td>10.0</td><td>9a89b36c</td><td>d8fc04df</td><td>b009d929</td><td>c7043c4b</td><td>25c83c98</td><td>7e0ccccf</td><td>bb3b7ab9</td><td>0b153874</td><td>a73ee510</td><td>3b08e48b</td><td>90b202b5</td><td>3563ab62</td><td>3a9dafb8</td><td>07d13a8f</td><td>33e5b3c4</td><td>b688c8cc</td><td>8efede7f</td><td>cbadff99</td><td>21ddcdc9</td><td>5840adea</td><td>2754aaf1</td><td>null</td><td>c7dc6720</td><td>3b183c5c</td><td>010f6491</td><td>9a483882</td></tr><tr><td>0.0</td><td>null</td><td>0.0</td><td>56.0</td><td>1.0</td><td>4288.0</td><td>null</td><td>0.0</td><td>28.0</td><td>125.0</td><td>null</td><td>0.0</td><td>null</td><td>24.0</td><td>73bd393d</td><td>403ea497</td><td>2cbec47f</td><td>3e2bfbda</td><td>25c83c98</td><td>null</td><td>57b4bd89</td><td>37e4aa92</td><td>a73ee510</td><td>3b08e48b</td><td>8ca164ab</td><td>21a23bfe</td><td>ddd66ce1</td><td>07d13a8f</td><td>e3209fc2</td><td>587267a3</td><td>776ce399</td><td>a78bd508</td><td>21ddcdc9</td><td>a458ea53</td><td>c2a93b37</td><td>null</td><td>423fab69</td><td>1793a828</td><td>e8b83407</td><td>2fede552</td></tr><tr><td>1.0</td><td>3.0</td><td>1.0</td><td>null</td><td>null</td><td>73.0</td><td>1.0</td><td>3.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>null</td><td>null</td><td>05db9164</td><td>bdaedcf5</td><td>ebe4edfa</td><td>ebc42d91</td><td>25c83c98</td><td>7e0ccccf</td><td>366a171d</td><td>5b392875</td><td>a73ee510</td><td>87c7319e</td><td>1ed3ae25</td><td>e8c9d3fa</td><td>aaa80b97</td><td>07d13a8f</td><td>8165b5cf</td><td>2c52502a</td><td>07c540c4</td><td>7d461236</td><td>null</td><td>null</td><td>f8c88eda</td><td>c9d4222a</td><td>3a171ecb</td><td>1b256e61</td><td>null</td><td>null</td></tr><tr><td>1.0</td><td>1.0</td><td>9.0</td><td>4.0</td><td>0.0</td><td>5.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>1.0</td><td>null</td><td>0.0</td><td>68fd1e64</td><td>bc478804</td><td>c746b84d</td><td>13508380</td><td>4cf72387</td><td>fe6b92e5</td><td>9c8ed289</td><td>0b153874</td><td>a73ee510</td><td>d3cf8a36</td><td>a7b606c4</td><td>bfbf389c</td><td>eae197fd</td><td>07d13a8f</td><td>0af7c64c</td><td>1e4dde7c</td><td>1e88c74f</td><td>65a2ac26</td><td>55dd3565</td><td>b1252a9d</td><td>a3e8e99b</td><td>null</td><td>3a171ecb</td><td>45ab94c8</td><td>001f3601</td><td>c84c4aec</td></tr><tr><td>0.0</td><td>null</td><td>1.0</td><td>3.0</td><td>null</td><td>null</td><td>null</td><td>0.0</td><td>0.0</td><td>5.0</td><td>null</td><td>0.0</td><td>null</td><td>null</td><td>05db9164</td><td>09e68b86</td><td>559d9a99</td><td>c160a680</td><td>25c83c98</td><td>null</td><td>d7087b39</td><td>f504a6f4</td><td>7cc72ec2</td><td>3b08e48b</td><td>786751d8</td><td>1664a565</td><td>0b7f85d0</td><td>64c94865</td><td>91126f30</td><td>181a367a</td><td>2005abd1</td><td>5aed7436</td><td>5e1505a3</td><td>a458ea53</td><td>cc3b7265</td><td>null</td><td>32c7478e</td><td>d931d13b</td><td>e8b83407</td><td>01702271</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["# view raw test data\n%sql select * from testTable limit 10;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>I1</th><th>I2</th><th>I3</th><th>I4</th><th>I5</th><th>I6</th><th>I7</th><th>I8</th><th>I9</th><th>I10</th><th>I11</th><th>I12</th><th>I13</th><th>C1</th><th>C2</th><th>C3</th><th>C4</th><th>C5</th><th>C6</th><th>C7</th><th>C8</th><th>C9</th><th>C10</th><th>C11</th><th>C12</th><th>C13</th><th>C14</th><th>C15</th><th>C16</th><th>C17</th><th>C18</th><th>C19</th><th>C20</th><th>C21</th><th>C22</th><th>C23</th><th>C24</th><th>C25</th><th>C26</th></tr></thead><tbody><tr><td>10.0</td><td>15.0</td><td>4.0</td><td>7.0</td><td>348.0</td><td>18.0</td><td>66.0</td><td>30.0</td><td>520.0</td><td>2.0</td><td>6.0</td><td>null</td><td>7.0</td><td>05db9164</td><td>1cfdf714</td><td>25b53c72</td><td>13508380</td><td>f1d40cbe</td><td>null</td><td>90a2c015</td><td>0b153874</td><td>a73ee510</td><td>466a312b</td><td>66bfc5e8</td><td>4c70d733</td><td>3af886ff</td><td>07d13a8f</td><td>e439dd9b</td><td>2280219e</td><td>e5ba7672</td><td>9df49ecd</td><td>55dd3565</td><td>b1252a9d</td><td>1429d5ca</td><td>ad3062eb</td><td>3a171ecb</td><td>45ab94c8</td><td>cb079c2d</td><td>c84c4aec</td></tr><tr><td>null</td><td>40.0</td><td>25.0</td><td>7.0</td><td>10532.0</td><td>85.0</td><td>171.0</td><td>8.0</td><td>74.0</td><td>null</td><td>2.0</td><td>null</td><td>7.0</td><td>5a9ed9b0</td><td>1cfdf714</td><td>e626c3c7</td><td>0cd3e43c</td><td>25c83c98</td><td>7e0ccccf</td><td>7195046d</td><td>0b153874</td><td>a73ee510</td><td>61283720</td><td>4d8549da</td><td>68f0f743</td><td>51b97b8f</td><td>b28479f6</td><td>d345b1a0</td><td>bb9de190</td><td>e5ba7672</td><td>e88ffc9d</td><td>21ddcdc9</td><td>b1252a9d</td><td>e8f657eb</td><td>null</td><td>bcdee96c</td><td>544f6090</td><td>cb079c2d</td><td>f2e052c3</td></tr><tr><td>0.0</td><td>0.0</td><td>6.0</td><td>null</td><td>7983.0</td><td>7.0</td><td>44.0</td><td>1.0</td><td>5.0</td><td>0.0</td><td>3.0</td><td>1.0</td><td>null</td><td>be589b51</td><td>1cfdf714</td><td>560e18db</td><td>02ba716f</td><td>25c83c98</td><td>3bf701e7</td><td>84a150ff</td><td>0b153874</td><td>a73ee510</td><td>3b08e48b</td><td>e08f36b2</td><td>3b4996ae</td><td>422ba909</td><td>1adce6ef</td><td>f3002fbd</td><td>636770c4</td><td>27c07bd6</td><td>e88ffc9d</td><td>790f389c</td><td>a458ea53</td><td>f050a9cf</td><td>c9d4222a</td><td>423fab69</td><td>ebd002c5</td><td>cb079c2d</td><td>ed7a66ba</td></tr><tr><td>0.0</td><td>2.0</td><td>4.0</td><td>2.0</td><td>1481.0</td><td>69.0</td><td>3.0</td><td>44.0</td><td>228.0</td><td>0.0</td><td>2.0</td><td>0.0</td><td>2.0</td><td>75ac2fe6</td><td>1cfdf714</td><td>aa585663</td><td>c7577387</td><td>25c83c98</td><td>fe6b92e5</td><td>a3579031</td><td>a25968f2</td><td>a73ee510</td><td>1a5ba63e</td><td>1054ae5c</td><td>0ec848db</td><td>d7ce3abd</td><td>b28479f6</td><td>d345b1a0</td><td>eb8a8ef3</td><td>e5ba7672</td><td>e88ffc9d</td><td>efa3470f</td><td>a458ea53</td><td>c3ec1415</td><td>null</td><td>bcdee96c</td><td>904cf83e</td><td>cb079c2d</td><td>09b3050a</td></tr><tr><td>0.0</td><td>1672.0</td><td>10.0</td><td>1.0</td><td>4623.0</td><td>434.0</td><td>4.0</td><td>25.0</td><td>236.0</td><td>0.0</td><td>1.0</td><td>null</td><td>44.0</td><td>05db9164</td><td>8084ee93</td><td>d032c263</td><td>c18be181</td><td>25c83c98</td><td>7e0ccccf</td><td>5e64ce5f</td><td>1f89b562</td><td>a73ee510</td><td>4f1e4025</td><td>8b94178b</td><td>dfbb09fb</td><td>025225f2</td><td>b28479f6</td><td>b2ff8c6b</td><td>84898b2a</td><td>e5ba7672</td><td>52e44668</td><td>null</td><td>null</td><td>0014c32a</td><td>null</td><td>32c7478e</td><td>3b183c5c</td><td>null</td><td>null</td></tr><tr><td>0.0</td><td>1.0</td><td>45.0</td><td>0.0</td><td>5617.0</td><td>320.0</td><td>17.0</td><td>4.0</td><td>95.0</td><td>0.0</td><td>1.0</td><td>null</td><td>3.0</td><td>39af2607</td><td>4f25e98b</td><td>3fa80e7b</td><td>7c246ddc</td><td>25c83c98</td><td>fbad5c96</td><td>0b28577f</td><td>1f89b562</td><td>a73ee510</td><td>2f45a7d3</td><td>ccecf8bb</td><td>0ec9545e</td><td>e853b835</td><td>64c94865</td><td>40e29d2a</td><td>c0b3c99f</td><td>3486227d</td><td>7ef5affa</td><td>55dd3565</td><td>a458ea53</td><td>d99b40a7</td><td>null</td><td>3a171ecb</td><td>f9f8a955</td><td>9d93af03</td><td>d509cc5d</td></tr><tr><td>1.0</td><td>-1.0</td><td>9.0</td><td>11.0</td><td>921.0</td><td>58.0</td><td>57.0</td><td>34.0</td><td>327.0</td><td>1.0</td><td>9.0</td><td>0.0</td><td>11.0</td><td>1464facd</td><td>09e68b86</td><td>0b1059dc</td><td>82aab4aa</td><td>25c83c98</td><td>null</td><td>b87f4a4a</td><td>0b153874</td><td>a73ee510</td><td>e70742b0</td><td>319687c9</td><td>79323ec3</td><td>62036f49</td><td>07d13a8f</td><td>36721ddc</td><td>522dccf9</td><td>e5ba7672</td><td>5aed7436</td><td>f30f7842</td><td>a458ea53</td><td>89d51bd1</td><td>null</td><td>32c7478e</td><td>e80b80d5</td><td>e8b83407</td><td>df2e3596</td></tr><tr><td>null</td><td>-1.0</td><td>1.0</td><td>1.0</td><td>15007.0</td><td>1.0</td><td>9.0</td><td>1.0</td><td>1.0</td><td>null</td><td>2.0</td><td>null</td><td>1.0</td><td>5a9ed9b0</td><td>09e68b86</td><td>c94341ca</td><td>40a048a5</td><td>25c83c98</td><td>null</td><td>b87f4a4a</td><td>0b153874</td><td>a73ee510</td><td>e70742b0</td><td>319687c9</td><td>85ade081</td><td>62036f49</td><td>07d13a8f</td><td>36721ddc</td><td>4629ae0b</td><td>e5ba7672</td><td>5aed7436</td><td>b8868b4a</td><td>a458ea53</td><td>44f7c960</td><td>null</td><td>32c7478e</td><td>c7e31a5c</td><td>e8b83407</td><td>dcc9c5d3</td></tr><tr><td>0.0</td><td>0.0</td><td>2.0</td><td>null</td><td>1729.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>1.0</td><td>null</td><td>null</td><td>05db9164</td><td>09e68b86</td><td>6bed6a88</td><td>a058fdb5</td><td>25c83c98</td><td>null</td><td>f14f1abf</td><td>985e3fcb</td><td>a73ee510</td><td>1f20471e</td><td>7b5deffb</td><td>7658ab1c</td><td>269889be</td><td>07d13a8f</td><td>36721ddc</td><td>55fb7803</td><td>d4bb7bd8</td><td>5aed7436</td><td>449c7d20</td><td>a458ea53</td><td>f4b7b02b</td><td>null</td><td>32c7478e</td><td>63863fe2</td><td>e8b83407</td><td>4cc52399</td></tr><tr><td>null</td><td>7.0</td><td>4.0</td><td>1.0</td><td>11638.0</td><td>null</td><td>0.0</td><td>1.0</td><td>31.0</td><td>null</td><td>0.0</td><td>0.0</td><td>1.0</td><td>05db9164</td><td>1cfdf714</td><td>dfd5584d</td><td>c60f65b0</td><td>25c83c98</td><td>3bf701e7</td><td>a2bea6d8</td><td>0b153874</td><td>a73ee510</td><td>a30a7b4a</td><td>72a65bcc</td><td>6fcc4b21</td><td>b8fee572</td><td>f862f261</td><td>0e1257cc</td><td>4cbb4c83</td><td>e5ba7672</td><td>e88ffc9d</td><td>7c629f16</td><td>b1252a9d</td><td>3be3a723</td><td>ad3062eb</td><td>55dd3565</td><td>3fdb382b</td><td>cb079c2d</td><td>49d68486</td></tr></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Separate out nummeric and categorical as iCols and cCols\n# Define Column Types\niCols = [i for i in train_parquet.columns if re.search('I',i)]\ncCols = [i for i in train_parquet.columns if re.search('C',i)]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["# Get basic statistics and display as pandas dataframe\ntrainDF = train_parquet.select(iCols)\ntrainDF = trainDF.describe()\ntrainPDF = trainDF.toPandas().set_index(\"summary\").transpose()\ntrainPDF"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>summary</th>\n      <th>count</th>\n      <th>mean</th>\n      <th>stddev</th>\n      <th>min</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>I1</th>\n      <td>25047061</td>\n      <td>3.5024133170754044</td>\n      <td>9.42907640710507</td>\n      <td>0.0</td>\n      <td>5775.0</td>\n    </tr>\n    <tr>\n      <th>I2</th>\n      <td>45840617</td>\n      <td>105.84841979766546</td>\n      <td>391.45782268707086</td>\n      <td>-3.0</td>\n      <td>257675.0</td>\n    </tr>\n    <tr>\n      <th>I3</th>\n      <td>36001170</td>\n      <td>26.913041020611274</td>\n      <td>397.97258302273355</td>\n      <td>0.0</td>\n      <td>65535.0</td>\n    </tr>\n    <tr>\n      <th>I4</th>\n      <td>35903248</td>\n      <td>7.322680248873305</td>\n      <td>8.793230712645805</td>\n      <td>0.0</td>\n      <td>969.0</td>\n    </tr>\n    <tr>\n      <th>I5</th>\n      <td>44657500</td>\n      <td>18538.991664871523</td>\n      <td>69394.60184622345</td>\n      <td>0.0</td>\n      <td>2.3159456E7</td>\n    </tr>\n    <tr>\n      <th>I6</th>\n      <td>35588289</td>\n      <td>116.06185085211598</td>\n      <td>382.5664493712397</td>\n      <td>0.0</td>\n      <td>431037.0</td>\n    </tr>\n    <tr>\n      <th>I7</th>\n      <td>43857751</td>\n      <td>16.333130032135028</td>\n      <td>66.04975524511718</td>\n      <td>0.0</td>\n      <td>56311.0</td>\n    </tr>\n    <tr>\n      <th>I8</th>\n      <td>45817844</td>\n      <td>12.517042137556713</td>\n      <td>16.688884567787543</td>\n      <td>0.0</td>\n      <td>6047.0</td>\n    </tr>\n    <tr>\n      <th>I9</th>\n      <td>43857751</td>\n      <td>106.1098234380509</td>\n      <td>220.28309398648037</td>\n      <td>0.0</td>\n      <td>29019.0</td>\n    </tr>\n    <tr>\n      <th>I10</th>\n      <td>25047061</td>\n      <td>0.6175294977722137</td>\n      <td>0.6840505553977029</td>\n      <td>0.0</td>\n      <td>11.0</td>\n    </tr>\n    <tr>\n      <th>I11</th>\n      <td>43857751</td>\n      <td>2.7328343170173044</td>\n      <td>5.199070884811348</td>\n      <td>0.0</td>\n      <td>231.0</td>\n    </tr>\n    <tr>\n      <th>I12</th>\n      <td>10768965</td>\n      <td>0.9910356287721244</td>\n      <td>5.597723872237179</td>\n      <td>0.0</td>\n      <td>4008.0</td>\n    </tr>\n    <tr>\n      <th>I13</th>\n      <td>35903248</td>\n      <td>8.217461161174054</td>\n      <td>16.211932558173665</td>\n      <td>0.0</td>\n      <td>7393.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["We do not know the actual column names in these data sets. We do see columns with negative values and/or null values in numeric columns. Without knowing the actual column and what type of business data is stored, it is hard to assume to clean up the data. At this point we intend to leave the data as is if it is negative and convert all the integer column as float type. From the above table, it is clear that most or all of the integer columns have some form of standard deviation which indicates none of the columns are constant columns but there are few quasi-constant like I10, I11 which we will explore in detail in later sections"],"metadata":{}},{"cell_type":"code","source":["# Describe categorical columns\n# Number of valid values in each categorical column\ncatNonMissing = train_parquet.agg(*(count(col(c)).alias(c) for c in cCols))\n\n# Percent of NULL or NaN values in each categorical column\nnRows = train_parquet.count()\ncatNullPercent = train_parquet.select([(count(when(isnan(c) | col(c).isNull(), c))/nRows).alias(c) for c in cCols])\n\n# Number of unique values in each categorical variable:\ncatUnique = train_parquet.agg(*(countDistinct(col(c)).alias(c) for c in cCols))\n\n# Explore the number of null records for categorical features\ndef unionAll(*dfs):\n    \"\"\"\n    A function to join/ merge dataframes\n    \"\"\"\n    return reduce(DataFrame.unionAll, dfs)\n\n# Create categorical statistics and convert to pandas dataframe for viewing\nCatStats = unionAll(catNonMissing, catNullPercent, catUnique)\ncategoricalFeatureStats = CatStats.toPandas()\ncategoricalFeatureStats = categoricalFeatureStats.T\ncategoricalFeatureStats.columns = ['Non Null Record count','Percentage of Null records', 'Number of Unique records']\ncategoricalFeatureStats"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Non Null Record count</th>\n      <th>Percentage of Null records</th>\n      <th>Number of Unique records</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>C1</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>1460.0</td>\n    </tr>\n    <tr>\n      <th>C2</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>583.0</td>\n    </tr>\n    <tr>\n      <th>C3</th>\n      <td>44281144.0</td>\n      <td>0.034019</td>\n      <td>10131226.0</td>\n    </tr>\n    <tr>\n      <th>C4</th>\n      <td>44281144.0</td>\n      <td>0.034019</td>\n      <td>2202607.0</td>\n    </tr>\n    <tr>\n      <th>C5</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>305.0</td>\n    </tr>\n    <tr>\n      <th>C6</th>\n      <td>40299992.0</td>\n      <td>0.120867</td>\n      <td>23.0</td>\n    </tr>\n    <tr>\n      <th>C7</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>12517.0</td>\n    </tr>\n    <tr>\n      <th>C8</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>633.0</td>\n    </tr>\n    <tr>\n      <th>C9</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>C10</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>93145.0</td>\n    </tr>\n    <tr>\n      <th>C11</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>5683.0</td>\n    </tr>\n    <tr>\n      <th>C12</th>\n      <td>44281144.0</td>\n      <td>0.034019</td>\n      <td>8351592.0</td>\n    </tr>\n    <tr>\n      <th>C13</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>3194.0</td>\n    </tr>\n    <tr>\n      <th>C14</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>27.0</td>\n    </tr>\n    <tr>\n      <th>C15</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>14992.0</td>\n    </tr>\n    <tr>\n      <th>C16</th>\n      <td>44281144.0</td>\n      <td>0.034019</td>\n      <td>5461305.0</td>\n    </tr>\n    <tr>\n      <th>C17</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>C18</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>5652.0</td>\n    </tr>\n    <tr>\n      <th>C19</th>\n      <td>25667759.0</td>\n      <td>0.440065</td>\n      <td>2172.0</td>\n    </tr>\n    <tr>\n      <th>C20</th>\n      <td>25667759.0</td>\n      <td>0.440065</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>C21</th>\n      <td>44281144.0</td>\n      <td>0.034019</td>\n      <td>7046546.0</td>\n    </tr>\n    <tr>\n      <th>C22</th>\n      <td>10885544.0</td>\n      <td>0.762535</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>C23</th>\n      <td>45840617.0</td>\n      <td>0.000000</td>\n      <td>15.0</td>\n    </tr>\n    <tr>\n      <th>C24</th>\n      <td>44281144.0</td>\n      <td>0.034019</td>\n      <td>286180.0</td>\n    </tr>\n    <tr>\n      <th>C25</th>\n      <td>25667759.0</td>\n      <td>0.440065</td>\n      <td>104.0</td>\n    </tr>\n    <tr>\n      <th>C26</th>\n      <td>25667759.0</td>\n      <td>0.440065</td>\n      <td>142571.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["The output shows we have 26 string variables. There are missing values, negative values, hashed values and null values. All the string columns are hashed and hence no meaningful information can be extracted. At this point we are going to assume all the string variables as categorical variables. The percentage of null values across the columns ranges from 0% to 45%. We are looking at approximately 46 million records. With this sheer volume the current categorical column needs deeper analysis into how many can be truly considered as categorical. In this table we have also captured the number of unqiue values available in each categorical column. Under normal circumstancs we expect lower number of unique values for categorical columns. This criteo data set presents unique challenge with the categorical columns as we see some columns with several hundred or sometimes even thousands of unique categorical columns. We will have to further evaluate column by column on how to hanlde this type of scenario. Outcome variable based binning and variance threshold analysis are couple of things we will do next in the featture engineering to understand more about these categorical features and will make appropriate decisions."],"metadata":{}},{"cell_type":"markdown","source":["The target variable for this data set is a binary response of value either 0 or 1. Naturally, 1 means a click upon the ad impression and a 0 means the user failed to click upon impression. It's important to notice that the two classes are not balanced. The ratio is about 1:3 between label 1 and label 0, as shown by the histogram below. We need to look into this imbalance and ensure our training data is fully balanced so that the model has equal learning capabilites to learn about both click and non-click prediction features. Later we will do down-sampling and up-sampling technique to handle this imbalance. The good news is that all the records have either 0 or 1 and there ar no missing or null labels."],"metadata":{}},{"cell_type":"code","source":["# Describe target column\ntrain_parquet.groupby('label').count().show()\nprint(\"label is Null count: \"+ str(train_parquet.where(col(\"label\").isNull()).count()))\nprint(\"label is NaN count: \"+ str(train_parquet.where(isnan(col(\"label\"))).count()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+--------+\nlabel|   count|\n+-----+--------+\n  1.0|11745438|\n  0.0|34095179|\n+-----+--------+\n\nlabel is Null count: 0\nlabel is NaN count: 0\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["# Distribution plot of response variable\ny = train_parquet.select('label').toPandas()\ndisplay(sns.countplot(x = y['label'].to_numpy()).set_title('Response Variable in Train'))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Convert continuous variables to dataframe\ntrainiColsDF = train_parquet.select(iCols).na.drop().sample(False, 0.05).toPandas()\n\n# Plot continuous variables\nfig = plt.subplots(figsize = (20,20))\ni = 1\nfor header in trainiColsDF.columns:\n  plt.subplot(5,3,i)\n  x = trainiColsDF[[header]]\n  sns.distplot(x).set_title(header)\n  plt.plot()\n  i +=1"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["With the exception of I8 and I10 Most of the numeric variables are highly right skewed. In order to satisfy linear regression's normality assumption, we decided to log transform all numeric variables except for I8 and I10."],"metadata":{}},{"cell_type":"code","source":["# Compute the correlation matrix\ncorr = trainiColsDF.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["The correlation plot among numeric variables suggest that the most extreme correlation among numeric variables is around -0.40, which suggest that multicollinearity may not be such a big deal for this data set."],"metadata":{}},{"cell_type":"markdown","source":["# 4. Feature Pre-Processing"],"metadata":{}},{"cell_type":"code","source":["trainDataset = train_parquet\ntestDataset = test_parquet\n\n# Imputing numeric\nnum_imputer = Imputer(\n    strategy='mean',\n    inputCols=['I1','I2','I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13'], \n    outputCols=['I1','I2','I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13']\n)\n# Create Numerical feature Impute Transformer\nmodel = num_imputer.fit(trainDataset)\n\n# Impute train data \ntrainDataset = model.transform(trainDataset)\n\n# Impute test data with the output from train dataset\ntestDataset = model.transform(testDataset)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["For numeric variables, we chose to impute NULL values with the columns mean. Mean imputation works better than median imputation because some columns could have NULL as the median value."],"metadata":{}},{"cell_type":"code","source":["# Log transform numeric variables\n# set a small epsilon to avoid log(0) singularities\nepsilon = 4.0\n\n# transform listed numeric columns one-by-one\nfor col_name in ['I1','I2','I3', 'I4', 'I5', 'I6', 'I7', 'I9','I11', 'I12', 'I13']:  # I8 and I10 were not transformed\n    trainDataset = trainDataset.withColumn(col_name, round(log(col(col_name) + epsilon),4))\n    testDataset = testDataset.withColumn(col_name, round(log(col(col_name) + epsilon),4))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["In order to apply log-transformation, we had to first make sure that all numeric features are positive values. Column I2 is the only numeric feature that contains negative values and has a minimum of -3. Therefore, we applied a modified version of the log-transformation: \n\n$$ x_{new} = \\log(x + 4)$$"],"metadata":{}},{"cell_type":"code","source":["# sanity check for null values resulting from log transformation\ntrainDataset.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in ['I1','I2','I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13']]).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+---+---+---+---+---+---+---+---+---+---+---+---+\n I1| I2| I3| I4| I5| I6| I7| I8| I9|I10|I11|I12|I13|\n+---+---+---+---+---+---+---+---+---+---+---+---+---+\n  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n+---+---+---+---+---+---+---+---+---+---+---+---+---+\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["# Convert continuous variables to dataframe\ntrainiColsDF = trainDataset.select(iCols).sample(False, 0.05).toPandas()\n\n# Plot continuous variables\n\nfig = plt.subplots(figsize = (20,20))\ni = 1\nfor header in trainiColsDF.columns:\n  plt.subplot(5,3,i)\n  x = trainiColsDF[[header]]\n  sns.distplot(x).set_title(header)\n  plt.plot()\n  i +=1"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Compute the correlation matrix\ncorr = trainiColsDF.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["# 5. Algorithm Theory\n\nThe problem statement of predicting whether an ad impression results in a click event (positive) or a no-click event (negtive) lands itself naturally in the domain of classification. Essentially, each observation is an ad impression and the outcome of the ad impression is denoted with a 1 or a 0 for the case of positive outcome (click) or the case of negative outcome (no-click), respectively. The data set included a total of 39 features - 13 numeric and 26 categorical. Given the feature information for a single obeservation, the chosen machine learning algorithm should be able to provide a decision of whether it's likely to result in a positive outcome or negative outcome. \n\nTo tackle this binary classification problem, the team has chosen to evaluate three machine learning classification algorithms: \n0. Logistic Regression\n1. Random Forest Classification\n2. Gradient Boosted Tree Classification\n\n__Logistic Regression__\n\nThe mathematical formulation of Logistic Regression is as follows: \n$$ \\log( \\frac{p_i}{1-p_i} ) = \\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n $$ <br>\n\nWhere \\\\(p_i\\\\) represents the probability of a positive outcome for the ith obeservation. The left-hand-side of the equation, \\\\(\\log(\\frac{p_i}{1-p_i})\\\\), also called the log-odds, is assumed to have a linear relationship with the predictors. The \\\\(\\beta_i\\\\) coefficients are the parameters of the model, which will be estimated training data set. \n\nIntuitively speaking, the linear model is to estimate the log of the ratio between the probability resulting in a postive outcome and the probability resulting in a negative outcome. Through cross-validation on the training set, the best model parameters (\\\\(\\beta_i\\\\)) will be selected to use to predict on the final test data set. During the predicting phase, an extension to the previous mathematical formulation is applied: \n$$ \\hat{p_i} = \\frac{b^{\\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n}}{b^{\\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n} + 1} = \\frac{1}{1+b^{-(\\beta_0 + \\beta_1 X_1 + ... + \\beta_n X_n)}}$$ <br>\n\nThe above equation is useful to applied the estimated model parameters from the training set to the unseen data in the test set. \n\nThere are several assumptions associated with Logistic Regression: \n- the log-odds must be a linear combination of the features\n- there must be little multicollinearity among the features\n- observations to be independent from each other\n\nPractically speaking, Logistic Regression is able to handle both numeric and categorical variables. However, the categorical variables must be translated into dummy binary variables or otherwise known as one-hot encoded. \n\n__Random Forest Classification__\n\nRandom Forest classification is an extension of the ordinary decision tree method. Tree based methods do not require linearity in the feature space. In addition, tree based methods generalize well to both numeric and categorical features, without the need to perform feature engineering such as one-hot-encoding. \n\nRandom Forest model has two improvements over the ordinary decision tree model: \n0. Bagging: instead of creating a single decision tree, Random Forest creates several decision trees, each based on a random sample of the training set. The prediction is made by evaluating outcomes from all decision trees to reach an consensus. In the case of classification, the final prediction is based on the majority voting. \n1. Randomly select features to build sub-trees: this is another adaptation on top of having an ensemble of smaller trees. By randomly selecting which features to include, the model effectively decorrelate features and avoid the effect from some features dominate the model. \n\nThe mathematical formulation of Random Forest is as follows: \n\n$$ \\hat{f(x)} = \\frac{1}{B}\\sum\\_{b=1}^{B}{f_b(x)} $$ <br>\n\nWhere \\\\(B\\\\) denotes the number of subtrees included in the Random Forest.\n\n__Gradient Boosted Tree Classification__\n\nGradient Boosted Tree classification is yet another extension of the ordinary decision tree method. Similar to Random Forest, it does not require linearity in the feature space and can be generalized to both numeric and categorical variables. Unlike Random Forest, which builds multiple subtrees at the same time and achieves final prediction through consensus, Gradient Boosted Tree builds smaller subtrees sequentially. In other words, Gradient Boosted Tree creates a very \"shallow\" tree, then tries to fit another \"shallow tree\" on top of the difference between the predicted value from the previous tree and the true value. \n\nThe mathematical formulation of Random Forest is as follows: \n\n$$ \\hat{f(x)} = \\sum\\_{b=1}^{B}{\\lambda f_b(x)} $$ <br>\n\nWhere \\\\(B\\\\) denotes the number of sequential trees build on top of each other and \\\\(\\lambda\\\\) represents the shrinkage parameter which prevents the tree from becoming too big and avoids overfitting.\n\nIn the next several sections, we will explore all three options based on our training set. Depending on the model performance in both prediction efficacy and computation runtime, we may choose to hone in on a single method for further fine-tuning. \n\n__Train/Dev/Test Data Split__\n\nOnly the training data set is used for model selection and model tuning purpose. The test data set, given as part of the original .tar.gz file is reserved for the final stage to test the entire pipeline end-to-end. For each round of model training, we reserve 70% of the original training set for training and use the rest 30% as a dev data set. Model performance on the dev data set is used as the criteria to benchmark models against each other."],"metadata":{}},{"cell_type":"markdown","source":["# 6. Feature Engineering Using Baseline Model"],"metadata":{}},{"cell_type":"code","source":["# Column list assignment\n# In place to enable manual management of features fed through to next stages \nnumericColumns = iCols\ncategoricalColumns = cCols"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["In order to facilitate our feature engineering choices we have opted to use logistic regression to confirm whether any improvement has been achieved.\n\nWe chose logistic regression as our baseline model because of its versatility and interpretability. In addition to the logistic regression assumptions mentioned earlier there are two other pre-requisites to enable us to fit a model to the data:\n- No missing values\n- All values have to be numeric\n\nWe address the pre-requisites first and evaluate the incremental improvement of feature engineering choice subsequently."],"metadata":{}},{"cell_type":"code","source":["def preprocessData(sourceTrain, sourceTest, preprocessPipeDefinition):\n    \"\"\"\n    Pipeline transformer function. Transforms train and test data using supplied transformer pipeline.\n    The transformer pipeline is fitted to train data and applied to both train and test data.\n    \n    INPUTS:\n    sourceTrain - train data, must include label - target variable\n    sourceTest - test data, cannot include label - target variable\n    preprocessPipeDefinition - transformer pipeline\n    \n    OUTPUTS:\n    trainDF - transformed training data\n    testDF - transformed test data\n    \"\"\"\n    # EXECUTE Pre-Processing Pipeline\n    partialPipeline = Pipeline().setStages(preprocessPipeDefinition)\n    pipelineModel = partialPipeline.fit(sourceTrain)\n    \n    TrainDF = pipelineModel.transform(sourceTrain)\n    TestDF = pipelineModel.transform(sourceTest)\n\n    # Keep relevant columns\n    trainColumns = [\"label\", \"features\"]\n    testColumns = [\"features\"]\n    \n    TrainDF = TrainDF.select(trainColumns)\n    TestDF = TestDF.select(testColumns)\n  \n    return TrainDF, TestDF\n  \ndef splitTestDev(TrainDF):\n    \"\"\"\n    Split train data into training and dev dataset.\n    \n    INPUTS:\n    trainDF - transformed train data\n    \n    OUTPUTS:\n    trainData - training part of train dataset\n    devData - dev part of train dataset\n    \"\"\"\n  ### Randomly split data into training and test sets. set seed for reproducibility\n  (trainData, devData) = TrainDF.randomSplit([0.7, 0.3], seed=100)\n  \n  return trainData, devData\n\ndef baselineModel(trainData, devData):\n  \"\"\"\n  Fit a logistic regression model and return metrics.\n  \n  INPUTS:\n  trainData - training part of train dataset\n  devData - dev part of train dataset\n  \n  OUTPUTS:\n  metricAUC - AUC metric\n  metricLogLoss[0][0] - logLoss metric\n  metricPrecision - Precision metric\n  metricRecall - Recall metric\n  df_cm - confusion matrix\n  lrModel - fitted logistic regression model\n  \"\"\"\n  # Create LogisticRegression model\n  lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter = 10)\n  lrModel  = lr.fit(trainData)\n\n  # Check prediction AUC\n  predictions = lrModel.transform(devData)\n  \n  # Cache predictions\n  predictions.cache()\n  \n  # Confusion Matrix\n  TP = predictions[(predictions.label == 1) & (predictions.prediction == 1)].count()\n  TN = predictions[(predictions.label == 0) & (predictions.prediction == 0)].count()\n  FP = predictions[(predictions.label == 0) & (predictions.prediction == 1)].count()\n  FN = predictions[(predictions.label == 1) & (predictions.prediction == 0)].count()\n  array = [[TP, FP],[FN, TN]]\n  df_cm = pd.DataFrame(array, range(2), range(2))\n  \n  # Calculate Precision and Recall\n  metricPrecision = TP/(TP+FP)\n  print(\"Precision: \", metricPrecision)\n  metricRecall = TP/(TP+FN)\n  print(\"Recall: \", metricRecall)\n  \n  # Calculate metrics\n  metricAUC = bcEvaluator.evaluate(predictions)\n  print(\"AUC: \", metricAUC)\n  \n  #metricF1 = mcEvaluator.evaluate(predictions)\n  #print(\"mcEvaluator output: \", metricF1)\n  \n  metricLogLoss = evaluate_log_loss(predictions)\n  print(\"logloss: \", metricLogLoss[0][0])\n  \n  # Clear predictions from cache\n  predictions.unpersist()\n  \n  return metricAUC, metricLogLoss[0][0], metricPrecision, metricRecall, df_cm, lrModel\n\n# Evalutation Metric 1: areaUnderROC\nbcEvaluator = BinaryClassificationEvaluator() # Possible metrics: areaUnderROC (default), areaUnderPR\n\n# Evalutation Metric 2: f1\nmcEvaluator = MulticlassClassificationEvaluator() # Possible metrics: f1 (default), weightedPrecision, weightedRecall, accuracy\n\n# Evalute model using log loss\ndef evaluate_log_loss(df):\n    \"\"\"\n    Calculate logloss.\n    \n    INPUTS:\n    df - prediction dataframe\n    \n    OUTPUTS:\n    average log loss for the prediction data frame\n    \"\"\"\n    # Set a small epsilon to correct for extreme singularity\n    epsilon = 1e-16\n    \n    # Get only the first element from the probability dense vector\n    firstelement = udf(lambda v:float(v[1]),FloatType())\n    df = df.withColumn(\"p\",firstelement(\"probability\"))\n    \n    # Calculate log loss for each observation and taking the average\n    return df.select(df.p, df.label, when(df['label'] == 1.0, -log(df['p'] + epsilon)).otherwise(-log(1 - df['p'] + epsilon)).alias('log_loss')).agg(avg(col(\"log_loss\"))).take(1)\n  \ndef plotLrCharts(confusionMatrix, model):\n  \"\"\"\n  plot charts to evaluate baseline model improvement process.\n    \n  INPUTS:\n  confusionMatrix - confusion matrix dataframe\n  model - fitted logistic regression model\n  \"\"\"\n  confusionMatrix.columns = ['P','N']\n  confusionMatrix.index = ['P','N']\n\n  fig = plt.subplots(figsize = (30,15))\n \n  # Plot confusion matrix\n  plt.subplot(5,5,1)\n  sns.heatmap(confusionMatrix, annot=True, fmt=\"d\", annot_kws={\"size\": 16})\n  plt.ylabel('Predicted Value')\n  plt.xlabel('Actual Value')\n  plt.title('Confusion Matrix')\n  \n  # Beta coefficients\n  beta = np.sort(model.coefficients)\n  plt.subplot(5,5,2)\n  plt.plot(beta)\n  plt.ylabel('Beta')\n  plt.title('Beta coefficients')\n\n  # ROC Curve\n  trainingSummary = model.summary\n  roc = trainingSummary.roc.toPandas()\n  plt.subplot(5,5,3)\n  plt.plot(roc['FPR'],roc['TPR'])\n  plt.ylabel('False Positive Rate')\n  plt.xlabel('True Positive Rate')\n  plt.title('ROC Curve')\n\n  # Precision / Recall\n  pr = trainingSummary.pr.toPandas()\n  plt.subplot(5,5,4)\n  plt.plot(pr['recall'],pr['precision'])\n  plt.ylabel('Precision')\n  plt.xlabel('Recall')\n  plt.title('Precision vs Recall')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"code","source":["# Test Base Model\nsqlContext.clearCache()\n\nnumDataset = trainDataset.select(['label','I1','I2','I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13'])\nnumDatasetTest = testDataset.select(['I1','I2','I3', 'I4', 'I5', 'I6', 'I7', 'I8', 'I9', 'I10', 'I11', 'I12', 'I13'])\n\n# Transform all features into a vector using VectorAssembler\nbaseStages = []\nassembler = VectorAssembler(inputCols=iCols, outputCol=\"features\")\nbaseStages += [assembler]\n\nTrainDF, testData = preprocessData(numDataset, numDatasetTest, baseStages)  \ntrainData, devData = splitTestDev(TrainDF)\nmetricAUC, metricLogLoss, precision, recall, confusionMatrix, model = baselineModel(trainData, devData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Precision:  0.5800269038010547\nRecall:  0.13827393131709864\nAUC:  0.6957416959505038\nlogloss:  0.5227971411627269\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["# Plot baseline model output\nplotLrCharts(confusionMatrix, model)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["__Baseline Model__\n\nThe initial baseline model has AUC: 0.69 and loglogg: 0.52. The winning Kaggle model adressing the same problem achieved a 0.44463 logloss. The metrics above will be used to assess the impact of the feature engineering and modelling techniques."],"metadata":{}},{"cell_type":"code","source":["# Scale Numeric Features\nnumericStages = []\nnumeric_assembler = VectorAssembler(inputCols=numericColumns, outputCol=\"numeric_features\")\nscaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"features\", withStd=True, withMean=False)\nnumericStages += [numeric_assembler, scaler]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":45},{"cell_type":"code","source":["# Test Base Model\nTrainDF, testData = preprocessData(trainDataset, testDataset, numericStages) \ntrainData, devData = splitTestDev(TrainDF)\nmetricAUC, metricLogLoss, precision, recall, confusionMatrix, model = baselineModel(trainData, devData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Precision:  0.5795718341986148\nRecall:  0.1384240401899403\nAUC:  0.6957086542898033\nlogloss:  0.5228460006450292\n</div>"]}}],"execution_count":46},{"cell_type":"code","source":["# Plot baseline model output\nplotLrCharts(confusionMatrix, model)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["__Normalisation__\n\nIntroducing normalisation of numeric features does not seem to improve model metrics. In fact we can observe a minor deterioration of performance, at this point however we opt to retain this tranformation in case we may want to test an algorithm that would require scaling."],"metadata":{}},{"cell_type":"markdown","source":["One-hot encoding is generally the most common way of handling categorical variables. However several categorical features have over 1M unique values. This is prohibitive to us being able to apply a simple one-hot encoding approach.\n\nIn the following section we test three separate techniques:\n- Hashing\n- Binning the long tail of the categorical features and subsequent one hot encoding\n- Binning the long tail of the categorical features and dimensionality reduction using chi-square selector\n\nThe hashing approach enables us to retain all of the data granularity, however we lose the subsequent interpretability of data as it is not possible to reverse engineer the hasing process. \n\nIn order to enable the other techniques we have introduced binning. We define the percentage ratio limit for categorical feature values. Unless this limit is achieved the value is binned into 'Other' category."],"metadata":{}},{"cell_type":"code","source":["# Update spark settings to avoid network timeout error\nspark.conf.set(\"spark.network.timeout\", 800)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":50},{"cell_type":"code","source":["# Imputing Categorical Variables - required for OHE and Chi Square only\ntrainDataset = trainDataset.fillna('Missing')\ntestDataset = testDataset.fillna('Missing')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":51},{"cell_type":"code","source":["def binCategoricalOther(trainDataset, testDataset, cCols, perVal = 0.0001):\n  \"\"\"\n  Bin long tail of categorical features into 'Other' value. \n  Any category values that are not present in the train data at least 'perVal' % are binned to 'Other'.\n  The training dataset is used to create a list of valid values that meet the threshold. \n  This list is broadcasted and both train and test data are transformed.\n    \n  INPUTS:\n  trainDataset - train dataframe\n  testDataset - test dataframe\n  cCols - categorical columns to transform\n  perVal - percentage threshold below which feature values are binned into 'Other' category value\n    \n  OUTPUTS:\n  trainData - trainsformed train data\n  devData - transformed test data\n  \"\"\"\n  # Create dataframes that will be transformed\n  trainDataset_v2 = trainDataset\n  testDataset_v2 = testDataset\n  \n  # Transformation\n  for c in cCols:\n    # Define the list of values in a variable that meet the 'perVal' threshold\n    total = trainDataset.count()\n    valuesToKeep = trainDataset.groupby(c).agg((sparkcount(c)/total).alias('percent')).sort(desc(\"percent\"))\n    valuesToKeep = valuesToKeep.filter(valuesToKeep.percent > perVal ).sort(desc(\"percent\")).select(c).toPandas().values.tolist()\n    valuesToKeep = list(itertools.chain(*valuesToKeep))\n    \n    # Broadcast the list for transformation\n    sc.broadcast(valuesToKeep)\n    \n    # Transform the data\n    trainDataset_v2 = trainDataset_v2.withColumn(c, when(col(c).isin(valuesToKeep), col(c)).otherwise('Other'))\n    testDataset_v2 = testDataset_v2.withColumn(c, when(col(c).isin(valuesToKeep), col(c)).otherwise('Other'))\n    \n  return trainDataset_v2, testDataset_v2                                            "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":52},{"cell_type":"code","source":["# Pipeline stages\nhashStages = [] \n\n# Hash categorical variables\nhasher = FeatureHasher(inputCols=categoricalColumns,\n                       outputCol=\"hashed_features\")\nhashStages += [hasher]\n\n# Scale Numeric Features\nnumeric_assembler = VectorAssembler(inputCols=numericColumns, outputCol=\"numeric_features\")\nscaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\", withStd=True, withMean=False)\nhashStages += [numeric_assembler, scaler]\n\n# Transform all features into a vector using VectorAssembler\nassemblerInputs =  [\"scaled_numeric_features\"] + [\"hashed_features\"]\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nhashStages += [assembler]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":53},{"cell_type":"code","source":["### DEFINE PRE-PROCESSING PIPELINE \n# Pipeline stages\nOHEStages = [] \n\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n    \n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n    \n    # Add stages.  These are not run here, but will run all at once later on.\n    OHEStages += [stringIndexer, encoder]\n\n# Scale Numeric Features\nnumeric_assembler = VectorAssembler(inputCols=numericColumns, outputCol=\"numeric_features\")\nscaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\", withStd=True, withMean=False)\nOHEStages += [numeric_assembler, scaler]\n\n# Transform all features into a vector using VectorAssembler\nassemblerInputs =  [\"scaled_numeric_features\"] + [c + \"classVec\" for c in categoricalColumns]\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nOHEStages += [assembler]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":54},{"cell_type":"code","source":["# Pipeline stages\nchiStages = [] \n\nfor categoricalCol in categoricalColumns:\n    # Category Indexing with StringIndexer\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n\n    # Add stages.  These are not run here, but will run all at once later on.\n    chiStages += [stringIndexer]\n\n# Transform all features into a vector using VectorAssembler\nassemblerCatInputs = [c + \"Index\" for c in categoricalColumns]\nassemblerCat = VectorAssembler(inputCols=assemblerCatInputs, outputCol=\"featureVector\")\nchiStages += [assemblerCat]\n\n# Select categorical features\nselector = ChiSqSelector(numTopFeatures=13, featuresCol=\"featureVector\", outputCol=\"selectedFeatures\", labelCol=\"label\")\nchiStages += [selector]\n\n# Scale Numeric Features\nnumeric_assembler = VectorAssembler(inputCols=numericColumns, outputCol=\"numeric_features\")\nscaler = StandardScaler(inputCol=\"numeric_features\", outputCol=\"scaled_numeric_features\", withStd=True, withMean=False)\nchiStages += [numeric_assembler, scaler]\n\n# Transform all features into a vector using VectorAssembler\nassemblerInputs =  [\"scaled_numeric_features\"] + [\"selectedFeatures\"]\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nchiStages += [assembler]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":55},{"cell_type":"code","source":["# hashing performance test\n# Preprocessing stage\nTrainDF, testData = preprocessData(trainDataset, testDataset, hashStages)\n# Split data\ntrainData, devData = splitTestDev(TrainDF)\n# Fit model and calculate metrics\nmetricAUC, metricLogLoss, precision, recall, confusionMatrix, model = baselineModel(trainData, devData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Precision:  0.6488652479935768\nRecall:  0.335771438496236\nAUC:  0.7852913470796927\nlogloss:  0.4635628132584069\n</div>"]}}],"execution_count":56},{"cell_type":"code","source":["# Plot baseline model output\nplotLrCharts(confusionMatrix, model)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["__Feature Hashing__\n\nExcept for recall, introducing hashed categorical features improves almost all metrics that we have decided to monitor. Hashing categorical features improves our 'True positives' significantly, for every 2.4 'True positives' however, the False positives increase by 1."],"metadata":{}},{"cell_type":"code","source":["# OHE performance test\n# Bin categorical variables\ntrainDataset_v2, testDataset_v2 = binCategoricalOther(trainDataset, testDataset, cCols, 0.0001)\n# Preprocessing stage\nTrainDF, testData = preprocessData(trainDataset_v2, testDataset_v2, OHEStages)\n# Split data\ntrainData, devData = splitTestDev(TrainDF)\n# Fit model and calculate metrics\nmetricAUC, metricLogLoss, precision, recall, confusionMatrix, model = baselineModel(trainData, devData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Precision:  0.6470380722867912\nRecall:  0.29927629937930816\nAUC:  0.7731870527884\nlogloss:  0.4728585093889319\n</div>"]}}],"execution_count":59},{"cell_type":"code","source":["# Plot baseline model output\nplotLrCharts(confusionMatrix, model)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["__One-Hot Encoding__\n\nLike hashing, introduction of one-hot encoded categorical variables results in significant improvement of our baseline model. The impact on the monitored metrics is consistent with the hashing approach."],"metadata":{}},{"cell_type":"code","source":["# Chi Square performance test\n# Bin categorical variables\ntrainDataset_v2, testDataset_v2 = binCategoricalOther(trainDataset, testDataset, cCols, 0.0001)\n# Preprocessing stage\nTrainDF, testData = preprocessData(trainDataset_v2, testDataset_v2, chiStages)\n# Split data\ntrainData, devData = splitTestDev(TrainDF)\n# Fit model and calculate metrics\nmetricAUC, metricLogLoss, precision, recall, confusionMatrix, model = baselineModel(trainData, devData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Precision:  0.5793961532157386\nRecall:  0.13341615915043187\nAUC:  0.6976338326947286\nlogloss:  0.5220854088097512\n</div>"]}}],"execution_count":62},{"cell_type":"code","source":["# Plot baseline model output\nplotLrCharts(confusionMatrix, model)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["__Chi-Square Selector__\n\nIn order to enable chi selector the categorical variable threshold had to be raised higher to 0.1% in comparison to One hot encoding 0.01%. In addition to the binning process we have initially selected 13 features to be selected as part of the process. While this approach may have helped us simplify our model it did not result in improvement of the baseline model and therefore we have made the decision not to incorporate and evaluate this feature engineering technique any further.\n\n__Categorical Feature Engineering Technique Conclusion__\n\nOut of the three techniques Hashing and One Hot Encoding resulted in significant improvement of the baseline model. The hashing technique however, while resulting in worse recall than the One Hot encoding, delivers 128K extra 'True positives'. It is therefore we have decided to move forward with the hashing technique.\n\nFashing has a significant impact on model interpretabillity. It is not possible to reverse engineer the impact of hashing. Should the interpretability of the feature values be important in order to help business drive commercial decisions one hot encoding would be the preferred option."],"metadata":{}},{"cell_type":"markdown","source":["## 7. Model Training and Selection\n\nWe noted in the previous EDA section that the outcome class is not balanced for the CTR data. Recall the ratio between positive observations (11 millions) and negative observations (34 millions) is roughly 1 to 3. The class imbalance issue could impact model performance and results in poor predictive power in the minority class. In the CTR data, the issue is very relevant because positive label is the minority class. \n\nTo correct for this issue, we tried two different methods: \n\n__Up Sample Minority Class__\n\nUpward sampling is a method to sample with replacement repeatedly from the minority class to match with the total observations in the majority class. For our training set, we sampled the positive observations in the training set to bring the total number to around 34 millions. Intuitively, it means that roughly each positive observation is used at least three times in model training. \n\n__Down Sample Majority Class__\n\nDownward sampling is the opposite of upward sampling - instead of adjusting minority class to match with the majority class, alternatively, we can sub-sample the majority class to match with the minority class. In this case, we sampled 11 out of 34 millions of the negative observations in the training set. Intuitively, it means that we are potentially leaving out 2/3 of the information in the negative class."],"metadata":{}},{"cell_type":"code","source":["# Supply hashed data into the rebalancing stage\nTrainDF, testData = preprocessData(trainDataset, testDataset, hashStages)\n\n# Define class count\nclassCount = TrainDF.select('label').groupBy('label').count().take(3)\nminorClassCount = classCount[0][1]\nminorClassDF = TrainDF.filter(TrainDF['label'] == 1)\n\nmajorClassCount = classCount[1][1]\nmajorClassDF = TrainDF.filter(TrainDF['label'] == 0)\n\n### Adding a classWeight column to the training data set\n# Define class weights for minor and major class\nmajorClassWeight = 1\nminorClassWeight = majorClassCount/minorClassCount\n\n# Adding a weight columns to preppedTrainDF\nrandGen = lambda x: majorClassWeight if x == 0 else minorClassWeight\nudfRandGen = udf(randGen, FloatType()) \nTrainDF = TrainDF.withColumn(\"classWeight\", udfRandGen(\"label\"))\n\n### Create new training data sets with downsampled majority class or upsampled minority class\n# Calculate the original class size\nprint(\"Distribution of 1 and 0 cases in the original training set is: \\n\", TrainDF.select('label').groupBy('label').count().take(3))\n\n# Downsampling Class 0\nTrainDownSampleDF = majorClassDF.sample(False, minorClassCount/majorClassCount).unionAll(minorClassDF)\nprint(TrainDownSampleDF.select('label').groupBy('label').count().take(3))\n\n# Upsampling Class 1\nTrainUpSampleDF = minorClassDF.sample(True, majorClassCount/minorClassCount).unionAll(majorClassDF)\nprint(TrainUpSampleDF.select('label').groupBy('label').count().take(3))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Distribution of 1 and 0 cases in the original training set is: \n [Row(label=1.0, count=11745438), Row(label=0.0, count=34095179)]\n[Row(label=1.0, count=11745438), Row(label=0.0, count=11742491)]\n[Row(label=1.0, count=34092500), Row(label=0.0, count=34095179)]\n</div>"]}}],"execution_count":66},{"cell_type":"code","source":["# Split data\ntrainData, devData = splitTestDev(TrainUpSampleDF)\n# Fit model and calculate metrics\nmetricAUC, metricLogLoss, precision, recall, confusionMatrix, model = baselineModel(trainData, devData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Precision:  0.7165655789812182\nRecall:  0.7063155796914113\nAUC:  0.7872957096906522\nlogloss:  0.5560379741127929\n</div>"]}}],"execution_count":67},{"cell_type":"code","source":["# Plot baseline model output\nplotLrCharts(confusionMatrix, model)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["# Split data\ntrainData, devData = splitTestDev(TrainDownSampleDF)\n# Fit model and calculate metrics\nmetricAUC, metricLogLoss, precision, recall, confusionMatrix, model = baselineModel(trainData, devData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Precision:  0.7142782638846105\nRecall:  0.7076532130219713\nAUC:  0.7859276472814759\nlogloss:  0.5576421857898269\n</div>"]}}],"execution_count":69},{"cell_type":"code","source":["# Plot baseline model output\nplotLrCharts(confusionMatrix, model)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["__Sampling Conclusion__\n\nBecause of the sampling approach the confusion matrix, precision and recall are not directly comparable with the previously reported metrics. \n\nThe AUC has improved by 0.2% - Up Sampling and 0.06% - Down Sampling, however the logloss for each of the options has deteriorated and therefore we do not use this technique in our final solution."],"metadata":{}},{"cell_type":"code","source":["# Create RandomForest model\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\nrfModel = rf.fit(trainData.sample(False, 0.05)) # => update to full set\npredictions = rfModel.transform(devData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":72},{"cell_type":"code","source":["# Cache predictions\npredictions.cache()\n\n# Calculate metrics\nmetricAUC = bcEvaluator.evaluate(predictions)\nprint(\"AUC: \", metricAUC)\n\n#metricF1 = mcEvaluator.evaluate(predictions)\n#print(\"mcEvaluator output: \", metricF1)\n\nmetricLogLoss = evaluate_log_loss(predictions)\nprint(\"logloss: \", metricLogLoss[0][0])\n\n# Clear predictions from cache\npredictions.unpersist()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">AUC:  0.6009146272647449\nlogloss:  0.5665781282947683\nOut[331]: DataFrame[label: float, features: vector, rawPrediction: vector, probability: vector, prediction: double]</div>"]}}],"execution_count":73},{"cell_type":"code","source":["# Create GBT model\ngbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\ngbtModel = gbt.fit(trainData.sample(False, 0.05)) # => update to full set\npredictions = gbtModel.transform(devData)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":74},{"cell_type":"code","source":["# Cache predictions\npredictions.cache()\n\n# Calculate metrics\nmetricAUC = bcEvaluator.evaluate(predictions)\nprint(\"AUC: \", metricAUC)\n\n#metricF1 = mcEvaluator.evaluate(predictions)\n#print(\"mcEvaluator output: \", metricF1)\n\nmetricLogLoss = evaluate_log_loss(predictions)\nprint(\"logloss: \", metricLogLoss[0][0])\n\n# Clear predictions from cache\npredictions.unpersist()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">AUC:  0.7332528081254357\nlogloss:  0.5026433538619681\nOut[333]: DataFrame[label: float, features: vector, rawPrediction: vector, probability: vector, prediction: double]</div>"]}}],"execution_count":75},{"cell_type":"code","source":["# Additional libraries required for definition of custom pipeline stage\nfrom pyspark.ml import Transformer\nfrom pyspark.sql import DataFrame\n\n# Create customer Transformer to carry out log transform\nclass LogTransform(Transformer):\n    \"\"\"\n    A custom Transformer which log transforms specific columns.\n    \"\"\"\n    # Initialize transformer\n    def __init__(self, inputCols):\n        super(LogTransform, self).__init__()\n        self.iCols = inputCols\n        \n    # Log transform function    \n    def _transform(self, dataDF: DataFrame) -> DataFrame:\n        for col_name in iCols:\n            dataDF = dataDF.withColumn(col_name, round(log(col(col_name) + 4.0),4))\n        \n        return dataDF\n\n# define Log stage for the pipeline\nnum_log = LogTransform(inputCols=['I1','I2','I3', 'I4', 'I5', 'I6', 'I7', 'I9','I11', 'I12', 'I13'])\n\n### End to End pipeline definition ###\n# Create LogisticRegression model - pipeline stage\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n\n# Define final end to end pipeline stages\nfinalStages = []\nfinalStages = [num_imputer, num_log, *hashStages] \nfinalStages += [lr]\n\n# Define pipeline\npipeline = Pipeline(stages=finalStages)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":76},{"cell_type":"code","source":["# clear cache\nsqlContext.clearCache()\n\n# reload source train data\ntrainDataset = train_parquet\n\n# Set dataframe partitions to enable efficient cross validation parallelism\n# Rule of thumb  Number of Cores / # Partitions = cross validation parallelism setting\nprint(trainDataset.rdd.getNumPartitions())\ntrainDataset = trainDataset.repartition(12)\nprint(trainDataset.rdd.getNumPartitions())\n\n# Cache training data\ntrainDataset.cache()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">83\n12\nOut[402]: DataFrame[label: float, I1: float, I2: float, I3: float, I4: float, I5: float, I6: float, I7: float, I8: float, I9: float, I10: float, I11: float, I12: float, I13: float, C1: string, C2: string, C3: string, C4: string, C5: string, C6: string, C7: string, C8: string, C9: string, C10: string, C11: string, C12: string, C13: string, C14: string, C15: string, C16: string, C17: string, C18: string, C19: string, C20: string, C21: string, C22: string, C23: string, C24: string, C25: string, C26: string]</div>"]}}],"execution_count":77},{"cell_type":"code","source":["# Define Parameter Grid for CV\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0, 0.05])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [25])\n             .build())\n\n# Cross Validation\ncvE2E = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=bcEvaluator, numFolds=5, parallelism = 2)\ncvModelE2E = cvE2E.fit(trainDataset)\n\n# Get predictions\npredictionsE2E = cvModelE2E.transform(trainDataset)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">MLlib will automatically track trials in MLflow. After your tuning fit() call has completed, view the MLflow UI to see logged runs.\n</div>"]}}],"execution_count":78},{"cell_type":"code","source":["predictionsE2E.cache()\n\n# Confusion Matrix\nTP = predictionsE2E[(predictionsE2E.label == 1) & (predictionsE2E.prediction == 1)].count()\nTN = predictionsE2E[(predictionsE2E.label == 0) & (predictionsE2E.prediction == 0)].count()\nFP = predictionsE2E[(predictionsE2E.label == 0) & (predictionsE2E.prediction == 1)].count()\nFN = predictionsE2E[(predictionsE2E.label == 1) & (predictionsE2E.prediction == 0)].count()\narray = [[TP, FP],[FN, TN]]\ndf_cm = pd.DataFrame(array, range(2), range(2))\n  \n# Calculate Precision and Recall\nmetricPrecision = TP/(TP+FP)\nprint(\"Precision: \", metricPrecision)\nmetricRecall = TP/(TP+FN)\nprint(\"Recall: \", metricRecall)\n  \n# Calculate metrics\nmetricAUC = bcEvaluator.evaluate(predictionsE2E)\nprint(\"AUC: \", metricAUC)\n  \nmetricLogLoss = evaluate_log_loss(predictionsE2E)\nprint(\"logloss: \", metricLogLoss[0][0])\n\npredictionsE2E.unpersist()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Precision:  0.6696567273659033\nRecall:  0.31596156737620174\nAUC:  0.7901983543343694\nlogloss:  0.4608243374895706\nOut[404]: DataFrame[label: float, I1: double, I2: double, I3: double, I4: double, I5: double, I6: double, I7: double, I8: double, I9: double, I10: double, I11: double, I12: double, I13: double, C1: string, C2: string, C3: string, C4: string, C5: string, C6: string, C7: string, C8: string, C9: string, C10: string, C11: string, C12: string, C13: string, C14: string, C15: string, C16: string, C17: string, C18: string, C19: string, C20: string, C21: string, C22: string, C23: string, C24: string, C25: string, C26: string, hashed_features: vector, numeric_features: vector, scaled_numeric_features: vector, features: vector, rawPrediction: vector, probability: vector, prediction: double]</div>"]}}],"execution_count":79},{"cell_type":"code","source":["# get best model\nbestModel = cvModelE2E.bestModel\n\n# Plot baseline model output\nplotLrCharts(df_cm, cvModelE2E)"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["# Get best parameters\nbestPipeline = cvModelE2E.bestModel\nbestLRModel = bestPipeline.stages[6]\nbestParams = bestLRModel.extractParamMap()\nbestParams"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[418]: {Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;aggregationDepth&#39;, doc=&#39;suggested depth for treeAggregate (&gt;= 2)&#39;): 2,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;elasticNetParam&#39;, doc=&#39;the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty&#39;): 0.0,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;family&#39;, doc=&#39;The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.&#39;): &#39;auto&#39;,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;featuresCol&#39;, doc=&#39;features column name&#39;): &#39;features&#39;,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;fitIntercept&#39;, doc=&#39;whether to fit an intercept term&#39;): True,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;labelCol&#39;, doc=&#39;label column name&#39;): &#39;label&#39;,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;maxIter&#39;, doc=&#39;maximum number of iterations (&gt;= 0)&#39;): 25,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;predictionCol&#39;, doc=&#39;prediction column name&#39;): &#39;prediction&#39;,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;probabilityCol&#39;, doc=&#39;Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities&#39;): &#39;probability&#39;,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;rawPredictionCol&#39;, doc=&#39;raw prediction (a.k.a. confidence) column name&#39;): &#39;rawPrediction&#39;,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;regParam&#39;, doc=&#39;regularization parameter (&gt;= 0)&#39;): 0.05,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;standardization&#39;, doc=&#39;whether to standardize the training features before fitting the model&#39;): True,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;threshold&#39;, doc=&#39;threshold in binary classification prediction, in range [0, 1]&#39;): 0.5,\n Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;tol&#39;, doc=&#39;the convergence tolerance for iterative algorithms (&gt;= 0)&#39;): 1e-06}</div>"]}}],"execution_count":81},{"cell_type":"code","source":["# Extract best hyperparameters\nbest_mod = cvModelE2E.bestModel\nparam_dict = best_mod.stages[-1].extractParamMap()\n\nsane_dict = {}\nfor k, v in param_dict.items():\n  sane_dict[k.name] = v\n\nbest_reg = sane_dict[\"regParam\"]\nbest_elastic_net = sane_dict[\"elasticNetParam\"]\nbest_max_iter = sane_dict[\"maxIter\"]\n\nprint(\"Best reg: \", best_reg)\nprint(\"Best elastic net parameters: \", best_elastic_net)\nprint(\"Best max Iterations: \", best_max_iter)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Best reg:  0.05\nBest elastic net parameters:  0.0\nBest max Iterations:  25\n</div>"]}}],"execution_count":82},{"cell_type":"code","source":["# Extract Metrics for each cross validation\ncvModelE2E.avgMetrics"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[420]: [0.7846310903086459,\n 0.7846233955948674,\n 0.7844952786388535,\n 0.7847712584250256,\n 0.6950311682278928,\n 0.6827879374915752]</div>"]}}],"execution_count":83},{"cell_type":"code","source":["# Parameter Grid used\nparamGrid"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[423]: [{Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;regParam&#39;, doc=&#39;regularization parameter (&gt;= 0).&#39;): 0.0,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;elasticNetParam&#39;, doc=&#39;the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.&#39;): 0.0,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 25},\n {Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;regParam&#39;, doc=&#39;regularization parameter (&gt;= 0).&#39;): 0.0,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;elasticNetParam&#39;, doc=&#39;the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.&#39;): 0.5,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 25},\n {Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;regParam&#39;, doc=&#39;regularization parameter (&gt;= 0).&#39;): 0.0,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;elasticNetParam&#39;, doc=&#39;the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.&#39;): 1.0,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 25},\n {Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;regParam&#39;, doc=&#39;regularization parameter (&gt;= 0).&#39;): 0.05,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;elasticNetParam&#39;, doc=&#39;the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.&#39;): 0.0,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 25},\n {Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;regParam&#39;, doc=&#39;regularization parameter (&gt;= 0).&#39;): 0.05,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;elasticNetParam&#39;, doc=&#39;the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.&#39;): 0.5,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 25},\n {Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;regParam&#39;, doc=&#39;regularization parameter (&gt;= 0).&#39;): 0.05,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;elasticNetParam&#39;, doc=&#39;the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.&#39;): 1.0,\n  Param(parent=&#39;LogisticRegression_f5ef01423767&#39;, name=&#39;maxIter&#39;, doc=&#39;max number of iterations (&gt;= 0).&#39;): 25}]</div>"]}}],"execution_count":84},{"cell_type":"code","source":["# Manually Record Model Results\nModels = [\"Model 1\",\n\"Model 2\",\n\"Model 3\",\n\"Model 4\",\n\"Model 5\",\n\"Model 6\"]\n\nRegParam = [0,0,0,0.5,0.5,0.5]\nElasticNetParam = [0,0.5,1.0,0,0.5,1.0]\nMaxIter = [25,25,25,25,25,25]\navgAUC = [0.7846310903086459, 0.7846233955948674, 0.7844952786388535, 0.7847712584250256, 0.6950311682278928, 0.6827879374915752]\n\n# Create a dataframe of model results\ndata = list(zip(Models, RegParam, ElasticNetParam, MaxIter, avgAUC))\nevalDF = pd.DataFrame(data, columns = [\"Model\", \"RegParam\", \"ElasticNetParam\", \"MaxIter\",\"avgAUC\"])\n\ndisplay(evalDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Model</th><th>RegParam</th><th>ElasticNetParam</th><th>MaxIter</th><th>avgAUC</th></tr></thead><tbody><tr><td>Model 1</td><td>0.0</td><td>0.0</td><td>25</td><td>0.7846310903086459</td></tr><tr><td>Model 2</td><td>0.0</td><td>0.5</td><td>25</td><td>0.7846233955948674</td></tr><tr><td>Model 3</td><td>0.0</td><td>1.0</td><td>25</td><td>0.7844952786388535</td></tr><tr><td>Model 4</td><td>0.5</td><td>0.0</td><td>25</td><td>0.7847712584250256</td></tr><tr><td>Model 5</td><td>0.5</td><td>0.5</td><td>25</td><td>0.6950311682278928</td></tr><tr><td>Model 6</td><td>0.5</td><td>1.0</td><td>25</td><td>0.6827879374915752</td></tr></tbody></table></div>"]}}],"execution_count":85},{"cell_type":"markdown","source":["# 8. Conclusions"],"metadata":{}},{"cell_type":"code","source":["# Generate predictions for entire dataset\n# Original intention was to submit the testDataset for logloss evaluation as our Heldout Dataset, however it does not seem to be possible to submit the predictions any longer.\ntestDataset = test_parquet\nfinalPredictions = bestModel.transform(testDataset)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":87},{"cell_type":"markdown","source":["In this project we explored three different classification models: Logistic Regression, Random Forest, and Gradient Boosted Tree. So far we have seen the most success in Logistic Regression in terms of both computational efficiency and prediction performance on the development dataset.\n\nHere are some basic statistics to compare the three modelling strategies: \n\n- Logistic Regression finished within 10 minutes on the entire training data set with hashed categorical variable and produced AUC of 0.785.\n- Random Forest took 32 minutes to finish on a 5% of the training data set and produced AUC of 0.600.\n- Gradient Boosted Tree took almost 8 hours to complete on a 5% sample of the training data set and produced AUC of 0.733. \n\nThe vast computational advantage is the reason why we chose logistic regression for further development, which includes testing categorical feature engineering and hyperparameter tuning.\n\nWe suspect that logistic regression has an advantage over tree-based methods on large scale data set because of its linearity in nature. Perhaps the assumption of the linearity in feature space, as well as the linear decision boundary allows highly optimized Stochastic Descent algorithm for coefficient estimation. On the other hand, we believe Random Forest and GBT have the potential to achieve higher performance, but require much more computation resources and take much longer for fine-tuing parameters to avoid overfitting. \n\nIn the end, our best Logistic Regression model is with L2 regularization and lambda = 0.5, which achieved 0.790 in AUC."],"metadata":{}},{"cell_type":"code","source":["# Manually Record Model Results\nModels = [\"LR: Numeric Only (Baseline Model)\",\n\"LR: Scaled Numeric\",\n\"LR: Numeric + Hashed Categorical\",\n\"LR: Numeric + OHE Categorical\",\n\"LR: Numeric + Chi-Square Selection\",\n\"LR: Numeric + Hashed Categorical + Up Sample\",\n\"LR: Numeric + Hashed Categorical + Down Sample\",\n\"RF: Numeric + Hashed Categorical (5% Sample)\",\n\"GBT: Numeric + Hashed Categorical (5% Sample)\",\n\"LR: Numeric + Hashed Categorical (Optimized) - DAG\",\n\"LR: Numeric + Hashed Categorical (Optimized)\" ]\n\nMinutes = [1.85,2.43,9.15,13.44,10.74,13.52,12.77,32.77,477,228,631]\nAUC = [0.695741696,0.695708654,0.785291347,0.773187053,0.697633833,0.78729571,0.785927647,0.600914627,0.733252808,0.78697426,0.7901983543343694]\nLogLoss = [0.522797141,0.522846001,0.463562813,0.472858509,0.522085409,0.556037974,0.557642186,0.566578128,0.502643354,0.462142691,0.4608243374895706]\nPrecision = [0.580026904,0.579571834,0.648865248,0.647038072,0.579396153,0.716565579,0.714278264,None,None,0.655837109,0.6696567273659033]\nRecall = [0.138273931,0.13842404,0.335771438,0.299276299,0.133416159,0.70631558,0.707653213,None,None,0.329772816,0.31596156737620174]\n\n# Create a dataframe of model results\ndata = list(zip(Models, Minutes, AUC, LogLoss, Precision, Recall))\nevalDF = pd.DataFrame(data, columns = [\"Model\", \"Minutes\", \"AUC\", \"LogLoss\",\"Precision\",\"Recall\"])\n\ndisplay(evalDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Model</th><th>Minutes</th><th>AUC</th><th>LogLoss</th><th>Precision</th><th>Recall</th></tr></thead><tbody><tr><td>LR: Numeric Only (Baseline Model)</td><td>1.85</td><td>0.695741696</td><td>0.522797141</td><td>0.580026904</td><td>0.138273931</td></tr><tr><td>LR: Scaled Numeric</td><td>2.43</td><td>0.695708654</td><td>0.522846001</td><td>0.579571834</td><td>0.13842404</td></tr><tr><td>LR: Numeric + Hashed Categorical</td><td>9.15</td><td>0.785291347</td><td>0.463562813</td><td>0.648865248</td><td>0.335771438</td></tr><tr><td>LR: Numeric + OHE Categorical</td><td>13.44</td><td>0.773187053</td><td>0.472858509</td><td>0.647038072</td><td>0.299276299</td></tr><tr><td>LR: Numeric + Chi-Square Selection</td><td>10.74</td><td>0.697633833</td><td>0.522085409</td><td>0.579396153</td><td>0.133416159</td></tr><tr><td>LR: Numeric + Hashed Categorical + Up Sample</td><td>13.52</td><td>0.78729571</td><td>0.556037974</td><td>0.716565579</td><td>0.70631558</td></tr><tr><td>LR: Numeric + Hashed Categorical + Down Sample</td><td>12.77</td><td>0.785927647</td><td>0.557642186</td><td>0.714278264</td><td>0.707653213</td></tr><tr><td>RF: Numeric + Hashed Categorical (5% Sample)</td><td>32.77</td><td>0.600914627</td><td>0.566578128</td><td>null</td><td>null</td></tr><tr><td>GBT: Numeric + Hashed Categorical (5% Sample)</td><td>477.0</td><td>0.733252808</td><td>0.502643354</td><td>null</td><td>null</td></tr><tr><td>LR: Numeric + Hashed Categorical (Optimized) - DAG</td><td>228.0</td><td>0.78697426</td><td>0.462142691</td><td>0.655837109</td><td>0.329772816</td></tr><tr><td>LR: Numeric + Hashed Categorical (Optimized)</td><td>631.0</td><td>0.7901983543343694</td><td>0.4608243374895706</td><td>0.6696567273659033</td><td>0.31596156737620174</td></tr></tbody></table></div>"]}}],"execution_count":89},{"cell_type":"code","source":["evalDF = evalDF.sort_values(by='AUC', ascending=True)\nplt.barh(evalDF.Model, evalDF.AUC, align='center', alpha=0.5)\nplt.title('Rank by AUC')"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["evalDF = evalDF.sort_values(by='LogLoss', ascending=False)\nplt.barh(evalDF.Model, evalDF.LogLoss, align='center', alpha=0.5)\nplt.title('Rank by LogLoss')"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["evalDF = evalDF.sort_values(by='Minutes', ascending=False)\nplt.barh(evalDF.Model, evalDF.Minutes, align='center', alpha=0.5)\nplt.title('Rank by Time Complexity')"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":["## \n\nThe best model in the kaggle leaderboard has a logloss of 0.44463. Our best model has a logloss of 0.460, a difference of approximately 0.015 compared with the leaderboard. If not for the PII obfuscation and column anonymization, we might have been able to better understand the columns and make better decisions during exploratory data analysis and data transformation, leading to better predictions.\n\nA next step would be to extend building the models to include other models such as Support Vector Machines, Naive Bayes, and ensemble methods. Naive Bayes does not work on negative feature values. It has been hard to understand why some of the feature values are negative without knowing more about the columns themselves. It would be help to learn more about the data set schema for a more meaningful, contextual setting of the features. \n\nThinking about the above model from a business impact perspective. There are a few business use cases that can leverage the above model. A few of them are listed below. \n1. Estimating the revenue potential from ads based on number of visitors to a website \n2. Prioritizing Criteo's customer businesses that have a higher potential for click through rate \n3. Dynamically pricing the cost of a click based on the click through rate. \n4. Recommending the placement of an ad based on the click through rate etc."],"metadata":{}},{"cell_type":"markdown","source":["## 8.4 Lessons Learned"],"metadata":{}},{"cell_type":"markdown","source":["__Feature Engineering__\n- Hashing is a fast and efficient way to deal with large categorical variables. It enabled us to retain all the information and led to better performance of the model both in terms of runtime as well as metrics.\n- For this particular use case and the techniques we have tried feature selection did not lead to model improvement. We have used Chi Square selector to try to select features based on their explanatory relationship with the target variable, however we have found the resulting metrics worse when compared to hashing.\n\n__Model selection__ <br>\nWe have used logistic regression throughout feature engineering. We have applied the preprocessing pipeline and tested random forest and gradient boosted trees, however the performance of each option has proven to be prohibitive. In the case of these two algorithms, perhaps feature reduction could lead to better and faster results.\n\n__Model hyperparamter tuning__<br>\nLogistic regression model had delivered distinctively better and faster results in comparison to random forest and gradient boosted trees. It is therefore we have selected this model for hyperparameter tuning exercise.\n\n__Training time__<br>\nOne iteration of our E2E pipeline takes approximately 12min. This is consistent with crossvalidation results where one iteration takes approximately 35min. Expected runtime of a 5-fold cross validation is approximately 1hr excluding any preprocessing.\nThe cluster workload can have a significant impact on the pipeline runtime: 10.5hrs vs 2.4 hrs.\n\n__Optimising the training time__<br>\nWe have tried a number of techinques to optimise the training time<br>\n- parallelism - using the parallelism setting we were able to execute two cross validations in parallel <br>\n<img src=\"https://github.com/Walekova/PipelineTuning/blob/master/Active%20jobs.png?raw=true\"/>\n- data partitioning - does not seem to have a favourable impact on training time. However the cluster was under heavy workload during training. <br>\n- cache - caching training does not lead to runtime improvement. <br>\n- DagCrossValidator - experimental cross validator module created by IBM. In theory this module should help spark manage memory and caching and could achieve up to 3.5 better performance by not having to redo same portions of the pipeline several times. However while it worked well for algorithm only pipelines it did not work for more complex pipelines. The module is about 2 years old and while we were able to fix some of the libraries there were other issues with deprecated python functions. <br>\n\n__Importance of training time__<br>\nBased on our research click models need to be retrained with relatively high frequency - normally weekly. This is to enable the models to capture changes in consumer behaviour on a timely basis. While it can be computationally expensive, retraining the models could be fully automated and minimise the effort required to redoply the new models. The retraining itself will require similar amount of time, unless more resources can be made available, however the resulting models can be used in a near real time architecture / scenario."],"metadata":{}},{"cell_type":"markdown","source":["# 9. Course Concepts"],"metadata":{}},{"cell_type":"markdown","source":["__Partitions__\n\n*Concept:* Partitioner class is used to partition data based on keys. The partition columns should be used frequently in queries for filtering and should have a small range of values with enough corresponding data to distribute the files in the directories. One wants to avoid too many small files, which make scans less efficient with excessive parallelism. You also want to avoid having too few large files, which can hurt parallelism.\n\n*Application:* Our data has been partitioned automatically by spark into 83 partitions. We have not adjusted the partitioning of the data as all of our operations have been carried out on the entire dataset and generally the values were not important in the queries. Furthermore based on preliminary EDA all of our variables were highly skewed, which would make it difficult for us to define a good partition key.\n\nWe have tried to however repartition our data down to 12 partitions using round robin partitioning, while in both cases the data did fit into memory, by monitoring we have noted that the data split into 12 partitions is more likely to be spilled to disk. The overall impact on single two fold cross validation model runtime was: <br>\n83 partitions - 35.5 min (1 set of hyperparameters), 24 min (2 set of hyperparameters)<br>\n12 partitions - 49 min (1 set of hyperparameters), 50 min (2 set of hyperparameters)<br>\n(note runtimes may not be directly comparable due to shared cluster resources)\n\nThe reason for repartitioning of data is a 'rule of thumb' statement from spark conference were cores / partitions = parallelism setting on crossvalidator. The cluster has 24 cores and we wanted to achieve parallelism = 2. \n\nIt is would seem that repartitioning data may not achieve the sought benefit, however it is difficult to draw any conclusions from the empirical evidence as for all except the 83 partition - 24 min run, the cluster was under heavy workload.\n\n__Caching__\n\n*Concept:* Storing data sets in a cluster-wide in-memory cache. This is useful when data is accessed repeatedly, such as running an iterative algorithm. Since operations in Spark are lazy, caching can help force computation.\n\n*Application:* We have tested caching as a way to improve performance.\n\n1. We load the predictions into memory and then calculate a number of metrics that the BinaryClassificationEvaluator does not capture. \n2. We load the training data into memory during cross validation. However the data has not remained in memory for the entire duration of training. The cached data has been moved between memory and disk throughout the training cycle.\n\nThe empirical evidence however would suggest that:\n- cashing predictions ahead of calculating all metrics improves the performance threefold.\n- caching training data does not improve the model runtime, in fact we have observed minor adverse impact.\n\nWe have investigated other options of leveraging caching however we stumbled across the following: (Source: https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-perf)\n'Native caching is effective with small data sets as well as in ETL pipelines where you need to cache intermediate results. However, Spark native caching currently doesn't work well with partitioning, since a cached table doesn't keep the partitioning data. A more generic and reliable caching technique is storage layer caching.' As a result, we have decided not to focus on further cache related performance tuning.\n\n__Broadcasting__\n\n*Concept:* Broadcast variables allow for a read-only variable to be cached on each machine / executor.\n\n*Application:* Our dataset is too large to broadcast any data. We had one instance were we were able to leverage this functionality - Feature binning. During the process of binning we analysed the train data and captured valid list of values for each feature. We have then broadcast this list and transformed the train data and test data.\n\n__DAGs__\n\n*Concept:* It is a scheduling layer in a spark which implements stage oriented scheduling. It converts logical execution plan to a physical execution plan. \n\n*Application:* We have focused on minimising any iterations and shuffles while constructing our final pipeline.\n\nThe DAG of our final pipeline: \n\n<img src=\"https://github.com/Walekova/PipelineTuning/blob/master/DAG_E2Epipeline.png?raw=true\"/>\n\n__Lazy evaluation__ \n\n*Concept:* An execution will not start until an action is triggered.\n\n*Application:* actions used extensively in this paper to trigger evaluation:\n- .show/ .reduce / .count <br>\n- .fit <br>"],"metadata":{}},{"cell_type":"markdown","source":["__Model Assumptions__\n\nIn this project, we have tested three types of models: Logistic Regression, Random Forest Classification, and Gradient Boosted Tree Classification. \n\nLogistic Regression requires the log-odds to be expressed as a linear combination fo the feature space. Although normalization is not required for running Logistic Regression, some research has suggested that normalizing numeric features improve performances of Stochastic Gradient Descent by putting all features on the same standard scale, which in turn improves efficiency in model training. In addition, Logistic Regression requires that there are minimal multicollinearity among features and that observations must be independent from one and other. \n\nIn contrast, the tree based methods make minimal assumptions on the dataset. In general tree based methods prefer categorical features over numeric features, because the model effectively is dividing up the feature spaces into partitions. Therefore, all numeric variables are effectively discretized prior to a split. In general Brieman's theorem is used to find the optimal splitting point for both numeric and categorical variables without having to evaluate all possible splitting points. Fortunately for us, the Spark's ML library provides many ready to use modules that tucks away the complexity."],"metadata":{}},{"cell_type":"markdown","source":["__Model Complexity and Bias-Variance Trade-off__:\n\nBias-variance trade-off is one of the key criteria throughout our project to balance model complexity. This concept is crucial in reducing the problem of overfitting. Overfitting happens when the model is trained on very specific data sets and produces good training results, but falls short when applied to unseen data. \n\nIn general, prediction error as a result of any machine learning model can be attributed to two sources: bias and variance. A complex model tend to produce large variance. For example, the two tree based models explored in this project have the tendency to overfit. Intuitively, overfitting in tree based method is to grow a very large and complex tree. In this case, the model results could be drastically different depending on the training data set and it does not generalize well on unseen data. In comparison, a simpler model such as the Logistic Regression tends to introduce more errors through bias. It makes rigid assumptions about the feature space: linear relationship, minimal multicollinearity, etc.\n\nHowever, in practice, even Logistic Regression can result in overfitting. To correct for this, we use the cross-validation to perform regularization and fine-tune the hyperparameters for Logistic Regression. The goal is to achieve a balanced bias and variance trade-off that results in a performant model that is also highly generalizable to unseen data. \n\n__Regularization__\n\nRegularization helps to control overfitting by penalizing complex models and effectively achieving feature selection by shrinking the coefficients of the not so useful features. When the coefficient becomes zero, the feature can be considered as eliminated. \n\nIn our project, we tried Lasso, Ridge and Elasticnet as regularization method and decided to go with Elasticnet for our CTR use case. With Elasticnet we get the best out of both Lasso and Ridge which is right for our CTR use case with the available data points and ambiguities with the categorical columns. For this project the raw data set provided on Kaggle contains hashed values for categorical variables. Most of the information is not discernible to human eyes. Therefore, we decided that it is best to leave the feature selection to proven methods like Elasticnet to determine which columns in the CTR dataset explains the variance. If we had known the column names, things might have changed from the EDA."],"metadata":{}},{"cell_type":"markdown","source":["#### Categorical Feature Engineering\n\n__Hashing__\n\n*Concept:* Fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix.\n\n*Application:* We have applied hashing to all of our categorical variables. This feature engineering has the following benefits:\n- it was the fastest out of the three techniques tested\n- it enable us to retain all of the granularity in the categorical variables\n- it achieved the best model outcome\n\n__Feature Binning__\n\n*Concept:* The original data values which fall into a given small interval, a bin, are replaced by a value representative of that interval.\n\n*Application:* We have tested the following scenarios, hashing all features and selecting features with less than 10000 values and used one hot encoding to evaluate the baseline model outcome. The hashing performed better. on the back of the first test we have worked with a hypethesis that the more granularity we can retain the better the metrics we will be able to achieve. \n\nIt is therefore we have chosen a very simple and fast binning method where only the long tail of the feature values were binned into 'Other' category. We have searched for lowest possible % threshold to apply to enable chi square selection and one hot encoding. Unlike other binning techniques, this does not require reverse enginnering to be applied when interpreting results.\n\n__String Index__\n\n*Concept:*  Encoding of a string column of labels to a column of label indices.\n\n*Application:* Pre-requisite technique to enable one hot encoding as well as Chi Square selector.\n\n__One-Hot Encoding__\n\n*Concept:* One-hot encoding is a methodology to convert categorical data into numeric data. One-hot encoding will create as many dummy variables as the number of categories.\n\n*Application:* After the binning process, we were able to one hot encode all features with all values that had at least 0.01% representation in the training dataset. This enabled us to achieve model performance similar to the hashing technique. \n\n__Feature Selection__\n\n*Concept:* Select features based on importance to the model.\n\n*Application:* We tested Chi-squared technique for determining feature relevance, the numTopFeatures chooses a fixed number of top features (tested on 13) according to a chi-squared test. - percentile is similar but chooses a fraction of all features instead of a fixed number. However we have found that to enable Chi Square selection we had to bin the long tail of the categorical variables, this preparation step had an adverse impact on the model outcome. Subsequent elimination of any of the features has further deteriorated our model metrics. Therefore we have decided not to proceed with Chi Square selection or any other feature reduction technique.\n\n#### Numeric Feature Engineering\n\n__Normalization__\n\n*Concept:* Method of standardizing features by removing the mean and scaling to unit variance.\n\n*Application:* In our CTR model, we scale the mean of the training data to 0 and standard deviation of the training data to 1. The algorithms we have chosen (logistic regression, random forest, gradient boosted trees) however do not require scaling, hence we have not observed any model improvement. We have decided to retain the transformation in our preprocessing pipeline to be able to leverage the pipeline at a later stage and be able to test other algorithms that require normalization to converge.\n\n__Log Transform__\n\n*Concept:* The log transformation can be used to make highly skewed distributions less skewed. \n\n*Application:* We have applied log transform to 11 of our 13 numeric variables, we have observed minor improvement 0.2% in model metrics. We have therefore decided to retain the transformation.\n\n#### Feature Engineering\n\n__Vector Embeddings__ \n\n*Concept:* Method of translating high dimensional vectors into low dimensional space.\n\n*Application:* Used extensively to translate categorical variable output of hashing, one hot encoding and chi square selection into low dimensional space to enable our models to process the data."],"metadata":{}},{"cell_type":"markdown","source":["# Bibliography\nhttps://www.youtube.com/watch?v=SLqKepl9rEoc <br>\nhttps://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2799933550853697/3911469456159528/2202577924924539/latest.html <br>\nhttps://databricks.com/session/model-parallelism-in-spark-ml-cross-validation <br>\nhttps://github.com/BryanCutler/PipelineTuning <br>\nJames G., Witten Daniela, Hastie T., & Tibshirani R. (2013). An Introduction to Statstical Learning."],"metadata":{}}],"metadata":{"name":"Criteo_Pipeline","notebookId":2791835342812779},"nbformat":4,"nbformat_minor":0}
